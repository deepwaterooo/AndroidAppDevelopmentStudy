#+latex_class: cn-article
#+title: Android OpenGL ES 渲染相关
#+author: deepwaterooo

* 通过SurfaceView，TextureView，GlSurfaceView显示相机预览
** SurfaceView，TextureView，GlSurfaceView的优缺点及区别
*** SurfaceView
- 继承自View,拥有View的大部分属性，但是由于holder的存在，不能设置透明度。
  - 优点：可以在一个独立的线程中进行绘制，不会影响主线程，使用双缓冲机制，播放视频时画面更流畅
  - 缺点：surface的显示不受View属性的控制，不能将其放在ViewGroup中，SurfaceView不能嵌套使用。
- SurfaceView预览相机视图不支持透明度，可以设置缩放旋转属性。如果需要做动画特效的话不推荐使用SurfaceView显示视图。可以使用TextureView或者GlSurfaceView来显示。
*** GlSurfaceView
- GlSurfaceView继承自SurfaceView类，专门用来显示OpenGL渲染的，简单理解可以显示视频，图像及3D场景这些的。
*** SurfaceTexture
- 和SurfaceView功能类似，区别是，SurfaceTexure可以不显示在界面中。
  - 使用OpenGl对图片流进行美化，添加水印，滤镜这些操作的时候我们都是通过SurfaceTexre去处理，处理完之后再通过GlSurfaceView显示。
  - 缺点，可能会导致个别帧的延迟。本身管理着BufferQueue,所以内存消耗会多一点。
*** TextureView
- 同样继承自View，必须在开启硬件加速的设备中使用（保守估计目前百分之九十的Android设备都开启了），TextureView通过setSurfaceTextureListener的回调在子线程中进行更新UI.
  - 优点：支持动画效果。
  - 缺点：在5.0之前在主线程渲染，在5.0之后在单独线程渲染。
|------------------+--------------------+------------------------|
|                  | TextureView        | SurfaceView            |
|------------------+--------------------+------------------------|
| 绘制             | 稍微延时           | 及时                   |
| 内存             | 高                 | 低                     |
| 动画             | 支持               | 不支持                 |
| 耗电             | 高                 | 低                     |
| 适用场景（推荐） | 视频播放，相机应用 | 大量画布更新(游戏绘制) |
|------------------+--------------------+------------------------|
** 如何通过SurfaceView显示Camera预览。
- 基本步骤
  - 在xml文件中设置SurfaceView 。
  - 实现SurfaceHolder.Callback的回调。
  - 打开摄像头Camera.open(0);
  - 设置摄像头相关参数；
  - 将摄像头数据设置到SurfaceView中，并开启预览。
#+BEGIN_SRC java
@Override
    public void surfaceCreated(SurfaceHolder holder) {
    try {
        mCamera = Camera.open(0); // Open the Camera in preview mode
        mCamera.setDisplayOrientation(90);
        mCamera.setPreviewDisplay(holder);
        mCamera.startPreview();
    } catch (IOException e) {
    }
}
@Override
    public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {
    mCamera.autoFocus(new Camera.AutoFocusCallback() {
            @Override
                public void onAutoFocus(boolean success, Camera camera) {
                if (success) {
                    mParameters = mCamera.getParameters();
                    mParameters.setPictureFormat(PixelFormat.JPEG); //图片输出格式
                    // mParameters.setFlashMode(Camera.Parameters.FLASH_MODE_TORCH);//预览持续发光
                    mParameters.setFocusMode(Camera.Parameters.FOCUS_MODE_CONTINUOUS_PICTURE);//持续对焦模式
                    mCamera.setParameters(mParameters);
                    mCamera.startPreview();
                    mCamera.cancelAutoFocus();
                }
            }
        });
}
@Override
    public void surfaceDestroyed(SurfaceHolder holder) {
    if (mCamera != null) {
        mCamera.stopPreview();
        mCamera.release();
        mCamera = null;
    }
}
#+END_SRC 
** 如何通过TextureView显示Camera预览
- TextureView和SurfaceView显示Camera数据其实差不多,差别就两点：
  - SurfaceView显示需要实现SurfaceHolder.Callback的回调而TextureView通过实现 TextureView.SurfaceTextureListener接口。
  - 当Camera使用SurfaceView预览时通过setPreviewDisplay(holder)方法来设置预览视图，而使用TextureView预览时使用setPreviewTexture(mCameraTextureView.getSurfaceTexture())方法来设置。
#+BEGIN_SRC java
public class CameraTextureViewShowActivity extends AppCompatActivity implements TextureView.SurfaceTextureListener {
    @BindView(R.id.camera_texture_view)
        TextureView mCameraTextureView;
    public Camera mCamera;
    @Override
        protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_camera_surface_texture);
        ButterKnife.bind(this);
        mCameraTextureView.setSurfaceTextureListener(this);
    }
    @Override
        public void onSurfaceTextureAvailable(SurfaceTexture surface, int width, int height) {
        try {
            mCamera = Camera.open(0);
            mCamera.setDisplayOrientation(90);
            mCamera.setPreviewTexture(mCameraTextureView.getSurfaceTexture());
            mCamera.startPreview();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
    @Override
        public void onSurfaceTextureSizeChanged(SurfaceTexture surface, int width, int height) {
    }
    @Override
        public boolean onSurfaceTextureDestroyed(SurfaceTexture surface) {
        if (mCamera != null) {
            mCamera.stopPreview();
            mCamera.release();
            mCamera = null;
        }
        return false;
    }
    @Override
        public void onSurfaceTextureUpdated(SurfaceTexture surface) {
    }
    @OnClick(R.id.btn_texture_anim)
        public void onViewClicked() {
        PropertyValuesHolder valuesHolder = PropertyValuesHolder.ofFloat("translationX", 0.0f, 0.0f);
        PropertyValuesHolder valuesHolder1 = PropertyValuesHolder.ofFloat("scaleX", 1.0f, 0.3f,1.0f);
        PropertyValuesHolder valuesHolder4 = PropertyValuesHolder.ofFloat("scaleY", 1.0f, 0.3f,1.0f);
        PropertyValuesHolder valuesHolder2 = PropertyValuesHolder.ofFloat("rotationX", 0.0f, 2 * 360.0f, 0.0F);
        PropertyValuesHolder valuesHolder5 = PropertyValuesHolder.ofFloat("rotationY", 0.0f, 2 * 360.0f, 0.0F);
        PropertyValuesHolder valuesHolder3 = PropertyValuesHolder.ofFloat("alpha", 1.0f, 0.7f, 1.0F);
        ObjectAnimator objectAnimator = ObjectAnimator.ofPropertyValuesHolder(mCameraTextureView, valuesHolder, valuesHolder1, valuesHolder2, valuesHolder3,valuesHolder4,valuesHolder5);
        objectAnimator.setDuration(5000).start();
    }
}
#+END_SRC 
** 如何通过GlSurfaceView处理Camera预览。
- 如果你在学习自定义相机，而且你的相机想要实现美颜，滤镜，人脸识别AR场景 and so on。这时候你就必须要学习如何使用GlsurfaView罗。如果你没有openGl的基本配置的知识或者你之前完全没有学习过openGl的开发，再次强烈建议你看一下这篇文章 Android openGl开发详解(一)——绘制简单图形，否则，下面内容可能会引起你的严重不适。
- 基本步骤
  - 在xml中添加GlSurfaceView
  - 创建渲染器类实现GlSurfaceView.Renderer
  - 清除画布，并创建一个纹理并绑定到。
  - 创建一个用来最后显示的SurfaceTexture来显示处理后的数据。
  - 创建Opengl ES程序并添加着色器到该程序中，创建openGl程序的可执行文件，并释放shader资源。
  - 打开摄像头，并配置相关属性。设置预览视图，并开启预览。
  - 添加程序到ES环境中，并设置及启用各类句柄。
  - 在onDrawFrame中进行画布的清理及绘制最新的数据到纹理图形中。
  - 设置一个SurfaceTexture.OnFrameAvailableListener的回调来通知GlSurfaceview渲染新的帧数据。
- 建议：GlSurfaceView作用简单的理解OpenGl对相机数据进行处理完之后的显示。我们需要明白的是渲染器的渲染周期及渲染方法的调用时机。
  - onSurfaceCreated()当surface创建(第一次进入当前页面)或者重新创建(切换后台再进入)的时候调用。
  - onSurfaceChanged()当surface大小发生改变的时候会被调用。
  - onDrawFrame()绘制当前帧数据的时候被调用。
- 大多数情况下渲染顶点着色器及片段着色器的代码会编写一个glsl的文件放到assets目录下进行访问。
- 下面是另外一个操作方式：
  - vertex_texture.glsl文件
#+BEGIN_SRC java
uniform mat4 textureTransform;
attribute vec2 inputTextureCoordinate;
attribute vec4 position;            //NDK坐标点
varying   vec2 textureCoordinate; //纹理坐标点变换后输出
void main() {
    gl_Position = position;
    textureCoordinate = inputTextureCoordinate;
}
#+END_SRC 
  - fragment_texture.glsl文件：
  #+BEGIN_SRC java
// extension GL_OES_EGL_image_external : require // comment ?
precision mediump float;
uniform samplerExternalOES videoTex;
varying vec2 textureCoordinate;
void main() {
    vec4 tc = texture2D(videoTex, textureCoordinate);
    float color = tc.r * 0.3 + tc.g * 0.59 + tc.b * 0.11;//这里进行的颜色变换处理，传说中的黑白滤镜。
    gl_FragColor = vec4(color,color,color,1.0);
}
  #+END_SRC 
  - 读取文件内容方式：
  #+BEGIN_SRC java
public static String read(Context context, String fileName) {
    String result = null;
    try {
        InputStream is = context.getResources().getAssets().open("Shader/" + fileName);
        int length = is.available();
        byte[] buffer = new byte[length];
        is.read(buffer);
        result = new String(buffer, "utf-8");
    } catch (IOException e) {
        e.printStackTrace();
    }
    return result;
}
  #+END_SRC 
- 具体实现在上面代码creatProgram()下注释掉通常做法的那部分。
- GLSurfaceView上的实现：
#+BEGIN_SRC java
@Override
    public void onSurfaceCreated(GL10 gl, EGLConfig config) {
    GLES20.glClearColor(0.0f, 0.0f, 0.0f, 0.0f);
    mSurfaceTexture = new SurfaceTexture(createOESTextureObject());
    creatProgram();
    // mProgram = ShaderUtils.createProgram(CameraGlSurfaceShowActivity.this, "vertex_texture.glsl", "fragment_texture.glsl");
    camera = Camera.open(camera_status);
    try {
        camera.setPreviewTexture(mSurfaceTexture);
        camera.startPreview();
    } catch (IOException e) {
        e.printStackTrace();
    }
    activeProgram();

}
@Override
    public void onSurfaceChanged(GL10 gl, int width, int height) {
    GLES20.glViewport(0, 0, width, height);
    Matrix.scaleM(mMVPMatrix,0,1,-1,1);
    float ratio = (float) width / height;
    Matrix.orthoM(mProjectMatrix, 0, -1, 1, -ratio, ratio, 1, 7);// 3和7代表远近视点与眼睛的距离，非坐标点
    Matrix.setLookAtM(mCameraMatrix, 0, 0, 0, 3, 0f, 0f, 0f, 0f, 1.0f, 0.0f);// 3代表眼睛的坐标点
    Matrix.multiplyMM(mMVPMatrix, 0, mProjectMatrix, 0, mCameraMatrix, 0);
}
@Override
    public void onDrawFrame(GL10 gl) {
    if (mBoolean){
        activeProgram();
        mBoolean = false;
    }
    if (mSurfaceTexture != null) { // <<<<<=====
        GLES20.glClear(GLES20.GL_COLOR_BUFFER_BIT | GLES20.GL_DEPTH_BUFFER_BIT);
        mSurfaceTexture.updateTexImage();
        GLES20.glUniformMatrix4fv(mMVPMatrixHandle, 1, false, mMVPMatrix, 0);
        GLES20.glDrawArrays(GLES20.GL_TRIANGLE_STRIP, 0, mPosCoordinate.length / 2);
    }
}
#+END_SRC 
- 关于前后摄像头的切换的问题，我上面的做法是在点击切换摄像头操作的时候只针对摄像头进行了释放重启操作，直接在onDrawFrame方法中对渲染矩阵进行了修改，没有对SurfaceTexture进行数据清除（具体看上面代码）。然而也看了一些主流的第三方Demo，这里不列出名字了。他们的做法是摄像头和surfaceTexture一块释放。当然，两种方式都可以，我上面的那种方式暂时没找到什么问题，而且我通过实测比第二种方式看到相机数据的时间要快一点。
- 做相机项目，最好能将每个步骤都弄清楚，逻辑理清楚了会节省很大一部分时间
** 视频录制
- 请求相机功能
  - 如果您的应用程序的基本功能涉及到 拍照，请将其在Google Play上的可见性限制为具有相机的设备。 以声明您的应用程序依赖于摄像头，请在清单文件中放置<uses-feature>标记。
#+BEGIN_SRC xml
<manifest ... >
    <uses-feature android:name="android.hardware.camera"
                  android:required="true" />
</manifest>
#+END_SRC
- 使用相机应用录制视频
#+BEGIN_SRC java
const val REQUEST_VIDEO_CAPTURE = 1
private fun dispatchTakeVideoIntent() {
    Intent(MediaStore.ACTION_VIDEO_CAPTURE).also { takeVideoIntent ->
        takeVideoIntent.resolveActivity(packageManager)?.also {
            startActivityForResult(takeVideoIntent, REQUEST_VIDEO_CAPTURE)
        }
    }
}
#+END_SRC 
- 观看视频
#+BEGIN_SRC java
override fun onActivityResult(requestCode: Int, resultCode: Int, intent: Intent) {
    if (requestCode == REQUEST_VIDEO_CAPTURE && resultCode == RESULT_OK) {
        val videoUri: Uri = intent.data
        mVideoView.setVideoURI(videoUri)
    }
#+END_SRC 

* 渲染步骤
- 初始化 EGL 环境大概分了 6 个步骤，在代码中都标记出来了。我们一个个来看。
  - 获得 EGLDisplay 对象，官网解释说是一个 display connection。可能其内部封装了连接设备显示器，获取显示器信息的方法。
  - 初始化 1 中获得的 display connection。这一步传入了 version 数组，作用是用来存放调用 eglInitialize 方法后获取的 EGL 的主版本和次版本。因为在 c 语言中一般是通过入参来传递返回值的，所以这里也是类似。
  - 获得显示器支持的图像缓冲配置，这里主要指定了各个颜色的深度和 alpha 通道的深度。然后调用 eglChooseConfig 方法后会返回多个支持我们指定配置的配置。这些配置会按匹配程度排序，数组第一个是最接近我们需要的配置。
  - 拿到配置后就可以创建 EGLContext，它为后续 OpenGL ES 渲染提供了上下文。
  - 创建 EGLSurface，已经有了 SurfaceView 了，这里为什么又来了一个 EGLSurface？其实 EGL 并不认识 SurfaceView，他只认识 EGLSurface，所以就用 EGLSurface 对 SurfaceView 中的 Surface 做了一层代理，实际上绘制还是绘制在 SurfaceView 中的 BufferQueue 中然后给屏幕进行显示的。
  - 最后一步即将 EGL 绑定到当前的 EGLSurface 上来，并指定了 OpenGL ES 的渲染上下文。
- 经过以上这 6 步，我们已经具备使用 OpenGL ES 进行渲染的能力了，下面来看下该怎么做。

- 首先在 SurfaceHolder 的 surfaceCreated 方法中调用上面的 initEGL，然后指定 OpenGL ES 的清屏颜色。
#+BEGIN_SRC java
@Override
    public void surfaceCreated(SurfaceHolder holder) {
        initEGL(holder);
        GLES32.glClearColor(1.0F, 0F, 0F, 1F);
    }
#+END_SRC 
- 接着在 surfaceChanged 回调中设置 OpenGL ES 中的视窗大小，并进行清屏操作。但仅仅这两步是无法将清屏颜色渲染到屏幕上的，因为此时只是将颜色渲染在了 EGLSurface 中的缓存中，另外还需要调用 EGL14.eglSwapBuffers 将缓存中的数据给到显示设备，这样才能渲染成功。
#+BEGIN_SRC java
@Override
public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {
    GLES32.glViewport(0, 0, width, height);
    GLES32.glClear(GLES32.GL_COLOR_BUFFER_BIT);
    EGL14.eglSwapBuffers(mEGLDisplay, mEGLSurface);
#+END_SRC 

** OpenGL fundamentals
- GLSurfaceView是一个视图，继承至SurfaceView，它内嵌的surface专门负责OpenGL渲染。
*** GLSurfaceView提供了下列特性：
- 1> 管理一个surface，这个surface就是一块特殊的内存，能直接排版到android的视图view上。
- 2> 管理一个EGL display，它能让opengl把内容渲染到上述的surface上。
- 3> 用户自定义渲染器(render)。
- 4> 让渲染器在独立的线程里运作，和UI线程分离。
- 5> 支持按需渲染(on-demand)和连续渲染(continuous)。
- 6> 一些可选工具，如调试。
*** 使用GLSurfaceView
- 通常会继承GLSurfaceView，并重载一些和用户输入事件有关的方法。如果你不需要重载事件方法，GLSurfaceView也可以直接使用，你可以使用set方法来为该类提供自定义的行为。例如，GLSurfaceView的渲染被委托给渲染器在独立的渲染线程里进行，这一点和普通视图不一样，setRenderer(Renderer)设置渲染器。
*** 初始化GLSurfaceView
- 初始化过程其实仅需要你使用setRenderer(Renderer)设置一个渲染器(render)。当然，你也可以修改GLSurfaceView一些默认配置。
  - * setDebugFlags(int)
  - * setEGLConfigChooser(boolean)
  - * setEGLConfigChooser(EGLConfigChooser)
  - * setEGLConfigChooser(int, int, int, int, int, int)
  - * setGLWrapper(GLWrapper) 
*** 定制android.view.Surface
- GLSurfaceView默认会创建像素格式为PixelFormat.RGB_565的surface。如果需要透明效果，调用getHolder().setFormat(PixelFormat.TRANSLUCENT)。透明(TRANSLUCENT)的surface的像素格式都是32位，每个色彩单元都是8位深度，像素格式是设备相关的，这意味着它可能是ARGB、RGBA或其它。
*** 选择EGL配置
- Android设备往往支持多种EGL配置，可以使用不同数目的通道(channel)，也可以指定每个通道具有不同数目的位(bits)深度。因此，在渲染器工作之前就应该指定EGL的配置。GLSurfaceView默认EGL配置的像素格式为RGB_656，16位的深度缓存(depth buffer)，默认不开启遮罩缓存(stencil buffer)。
- 如果你要选择不同的EGL配置，请使用setEGLConfigChooser方法中的一种。
*** 调试行为
- 你可以调用调试方法setDebugFlags(int)或setGLWrapper(GLSurfaceView.GLWrapper)来自定义GLSurfaceView一些行为。在setRenderer方法之前或之后都可以调用调试方法，不过最好是在之前调用，这样它们能立即生效。
*** 设置渲染器
- 总之，你必须调用setRenderer(GLSurfaceView.Renderer)来注册一个GLSurfaceView.Renderer渲染器。渲染器负责真正的GL渲染工作。
*** 渲染模式
- 渲染器设定之后，你可以使用setRenderMode(int)指定渲染模式是按需(on demand)还是连续(continuous)。默认是连续渲染。
*** Activity生命周期
- Activity窗口暂停(pause)或恢复(resume)时，GLSurfaceView都会收到通知，此时它的onPause方法和onResume方法应该被调用。这样做是为了让GLSurfaceView暂停或恢复它的渲染线程，以便它及时释放或重建OpenGL的资源。

** 事件处理 
- 系统默认mode==RENDERMODE_CONTINUOUSLY，这样系统会自动重绘；mode==RENDERMODE_WHEN_DIRTY时，只有surfaceCreate的时候会绘制一次，然后就需要通过requestRender()方法主动请求重绘。同时也提到，如果你的界面不需要频繁的刷新最好是设置成RENDERMODE_WHEN_DIRTY，这样可以降低CPU和GPU的活动，可以省电。
- 为了处理事件，一般都是继承GLSurfaceView类并重载它的事件方法。但是由于GLSurfaceView是多线程操作，所以需要一些特殊的处理。由于渲染器在独立的渲染线程里，你应该使用Java的跨线程机制跟渲染器通讯。queueEvent(Runnable)方法就是一种相对简单的操作。
- *这里关于多线程处理的部分，是之前自己不曾注意到过的，需要理解原理和加强*
-  (注：如果在UI线程里调用渲染器的方法，很容易收到“call to OpenGL ES API with no current context”的警告，典型的误区就是在键盘或鼠标事件方法里直接调用opengl es的API，因为UI事件和渲染绘制在不同的线程里。更甚者，这种情况下调用glDeleteBuffers这种释放资源的方法，可能引起程序的崩溃，因为UI线程想释放它，渲染线程却要使用它。)
#+BEGIN_SRC java
class MyGLSurfaceView extends GLSurfaceView { 
     private MyRenderer mMyRenderer; 
         public void start() { 
             mMyRenderer = ...; 
             setRenderer(mMyRenderer); 
         } 
         public boolean onKeyDown( int keyCode, KeyEvent event) { 
             if (keyCode == KeyEvent.KEYCODE_DPAD_CENTER) { 
                 queueEvent( new Runnable() { 
                     // 这个方法会在渲染线程里被调用 
                          public void run() { 
                              mMyRenderer.handleDpadCenter(); 
                          }}); 
                      return true ; 
                  } 
                  return super .onKeyDown(keyCode, event); 
             } 
       } 
}
#+END_SRC 
- 调用queueEvent就是给队列中添加runnable
#+BEGIN_SRC java
public void queueEvent(Runnable r) {
     synchronized (sGLThreadManager) {
         mEventQueue.add(r);
         sGLThreadManager.notifyAll();
     }
}
#+END_SRC 
- 在guardenRun()中有如下代码：
#+BEGIN_SRC java
 if (! mEventQueue.isEmpty()) {
     event = mEventQueue.remove( 0 );
     break ;
 }
 if (event != null ) {
     event.run();
     event = null ;
     continue ;
 }
#+END_SRC 
- 因为每次都会remove掉添加的runnable，所以上面那个demo就是非常好的解释，每次按键就是添加runnable。当然，这也是要求绘制是一直在循环重绘的状态才能看到效果。
- (注：如果在UI线程里调用渲染器的方法，很容易收到“call to OpenGL ES API with no current context”的警告，典型的误区就是在键盘或鼠标事件方法里直接调用opengl es的API，因为UI事件和渲染绘制在不同的线程里。更甚者，这种情况下调用glDeleteBuffers这种释放资源的方法，可能引起程序的崩溃，因为UI线程想释放它，渲染线程却要使用它。)
- 再举一下简单实用的小例子，关于queueEvent()的使用的
#+BEGIN_SRC java
public class ClearActivity extends Activity {
    private GLSurfaceView mGLView;
    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        // mGLView = new GLSurfaceView(this);
        mGLView = new ClearGLSurfaceView(this);
        mGLView.setRenderer(new ClearRenderer());
        setContentView(mGLView);
    }
     @Override
    protected void onPause() {
        super.onPause();
        mGLView.onPause();
    }
     @Override
     protected void onResume() {
        super.onResume();
        mGLView.onResume();
    }
}
class ClearGLSurfaceView extends GLSurfaceView {
    public ClearGLSurfaceView(Context context) {
        super(context);
        mRenderer = new ClearRenderer();
        setRenderer(mRenderer);
    }
    public boolean onTouchEvent(final MotionEvent event) {
        queueEvent(new Runnable(){
            public void run() {
                mRenderer.setColor(event.getX() / getWidth(),
                        event.getY() / getHeight(), 1.0f);
            }});
            return true;
        }
        ClearRenderer mRenderer;
}
class ClearRenderer implements GLSurfaceView.Renderer {
    private float mRed;
    private float mGreen;
    private float mBlue;
    public void onSurfaceCreated(GL10 gl, EGLConfig config) {
        // Do nothing special.
    }
    public void onSurfaceChanged(GL10 gl, int w, int h) {
        gl.glViewport(0, 0, w, h);
    }
    public void onDrawFrame(GL10 gl) {
        gl.glClearColor(mRed, mGreen, mBlue, 1.0f);
        gl.glClear(GL10.GL_COLOR_BUFFER_BIT | GL10.GL_DEPTH_BUFFER_BIT);
    }
    public void setColor(float r, float g, float b) {
        mRed = r;
        mGreen = g;
        mBlue = b;
    }
}
#+END_SRC 
- 这个应用每帧都在清楚屏幕。当你点击屏幕时，它清除颜色基于你触屏时间的X、Y坐标。注意在 ClearGLSurfaceView.onTouchEvent()中使用queueEvent()。queueEvent()方法被安全地用于在UI线程和渲染线程之间进行交流。如果你愿意，你还可以使用一些其他的java线程间交流技术，例如Renderer 类本身的同步方法。然而，queueing 事件经常是一种用于处理线程间信息交流的更简单方式。


* 顶点属性、顶点数组和缓冲区对象
** 顶点属性和顶点数组
- 顶点数组中包含了每一个顶点的属性，是保存在用户内核的缓冲区。(语文不好的跟我念：顶点数组是缓冲区，这个缓冲区在用户内核，里面存放的是每一个顶点的属性)。
*** 单个顶点属性
- 如果我们有单个的顶点需要处理的话，可以使用下列方法将一个顶点对象映射为一个索引，这样我们就可以在需要的地方使用索引而不是顶点对象去操作数据。OpenGL提供了下列方法去创建单个顶点对象。
#+BEGIN_SRC cpp
  void glVertexAttrib1f (GLuint index, GLfloat x);
  void glVertexAttrib1fv (GLuint index, const GLfloat *v);
  void glVertexAttrib2f (GLuint index, GLfloat x, GLfloat y);
  void glVertexAttrib2fv (GLuint index, const GLfloat *v);
  void glVertexAttrib3f (GLuint index, GLfloat x, GLfloat y, GLfloat z);
  void glVertexAttrib3fv (GLuint index, const GLfloat *v);
  void glVertexAttrib4f (GLuint index, GLfloat x, GLfloat y, GLfloat z, GLfloat w);
  void glVertexAttrib4fv (GLuint index, const GLfloat *v);
#+END_SRC 
*** 顶点数组
    #+BEGIN_SRC cpp
void glVertexAttribPointer (GLuint index, GLint size, GLenum type, GLboolean normalized, GLsizei stride, const void *pointer);
void glVertexAttribIPointer (GLuint index, GLint size, GLenum type, GLsizei stride, const void *pointer);
    #+END_SRC 
- 前一个方法允许任何数字类型数组，后一个方法只允许整型数组。
  - index：顶点索引，可由用户自行指定。
  - size：对于坐标来说不应该使用size(尺度)最贴切的是dimension(维度)，取值范围1~4，代表当前顶点使用的坐标系，是线性坐标系(只有x)，或是4维空间坐标系(x,y,z,w)。
  - type：数据格式，包括常用的数据类型，比如GL_FLOAT等。
  - normalized：是否规范化，对于使用glVertexAttribPointer方法的非GL_FLOAT数组，将次标记设置为真后，有符号数将被按比例映射到GL_FLOAT[-1.0，1.0]区间，无符号数将被按比例映射到[0，1.0]区间。
  - stride：偏移量，如果该数组不仅仅用于存放坐标的话，需要说明偏移量，否则设置为0即可。
  - ptr：用作缓冲区的数组
*** 启用和关闭顶点数组
- 如果我们使用index指定了一个顶点数组，那么我们可以使用以下两种方式选择开启或者关闭使用此顶点数组。
#+BEGIN_SRC cpp
  void glEnableVertexAttribArray (GLuint index);
  void glDisableVertexAttribArray (GLuint index);
#+END_SRC 
*** 布局限定符
- 一旦我们将一个索引与顶点数组绑定起来，我们就可以在着色器中自由的使用数组中的数据。这种方式使用起来最为方便，但是在某些情况下，我们可能还需要另外的方式(值得注意的是，这种绑定在下一次程序链接时生效)
#+BEGIN_SRC cpp
  void glBindAttribLocation (GLuint program, GLuint index, const GLchar *name);
#+END_SRC 
  - program：程序对象索引。
  - index：定点数组索引。
  - name：着色器中使用此数据的变量的名称。
- 例如：我们可以使用此方法修改7.4.1中的着色器
  #+BEGIN_SRC cpp
  glBindAttribLocation(program,0,"pos");//用户程序添加绑定
  in vec2 pos; //不需要布局限定符
  #+END_SRC 
** 顶点数组对象
- 使用顶点数组操作可能需要多次调用8.1中的方法来切换状态。实际上在OpenGL ES 3.0 中，同一时刻总是只有一个活动的顶点数组对象。那么我们可以通过更简单的方法去指定当前正在使用的顶点数组。使用方式就是记录一段你需要对该数组进行的操作，并保存到一个操作对象当中，然后再每次需要时调用此操作记录对象即可。
*** 创建顶点数组对象
    #+BEGIN_SRC cpp
void glGenVertexArrays (GLsizei n, GLuint *arrays);
    #+END_SRC 
- n：需要的顶点数组对象个数
- arrays：保存对象索引的数组
*** 绑定顶点数组对象
- 顶点数组对象的目的是简化操作，我们需要将一个数组对象和一系列关于顶点数组和缓存的操作绑定起来。当我们调用完毕下列方法的时候，其后所有关于顶点数组和缓存的操作都将被记录到顶点数组对象的状态当中。
#+BEGIN_SRC cpp
void glBindVertexArray (GLuint array);
#+END_SRC 
- array：需要使用或修改状态的顶点数组对象的索引。
*** 删除顶点数组对象
- 同理，当我们不再需要对一段顶点数组进行操作的时候，需要通知OpenGL删除关于此数组的操作记录。我们可以使用如下方法：
#+BEGIN_SRC cpp
void glDeleteVertexArrays (GLsizei n, const GLuint *arrays);
#+END_SRC 
- n：需要删除的对象个数。
- arrays：包含数组对象索引的数组。
** 顶点缓冲区对象
- 假如我们将数据存储到用户内存，那么每当我们调用绘图方法时，都需要将数据从用户内存复制到图形内存中。但是我们没有必要再每次绘图时都去复制数据。为了节省内存和电力开销，OpenGL ES 提供了顶点缓存技术。OpenGL ES 3.0支持两类缓存区对象：
  - GL_ARRAY_BUFFER：标志指定缓冲区用于保存顶点数据
  - GL_ELEMENT_ARRAY_BUFFER：标志指定缓冲区用于保存图元索引数据
*** 获取缓冲区索引
- 由于这个缓冲区需要在两个内核空间中共享，所以OpenGL不建议像创建顶点数组那样让用户自己去指定索引。OpenGL提供了一个函数用来获取当前空闲的缓冲区索引。
#+BEGIN_SRC cpp
void glGenBuffers (GLsizei n, GLuint *buffers);
#+END_SRC 
- n：需要得到的索引数量。
- buffers：保存索引的数组。
*** 绑定缓冲区
- 我们通过8.3.1获取到了可用的缓冲区索引，接下来我们需要将索引与缓冲区类型进行绑定。
#+BEGIN_SRC cpp
void glBindBuffer (GLenum target, GLuint buffer);
#+END_SRC 
- target：缓冲区存放的数据类型。
- buffer：已获取的缓冲区索引。
*** 绑定数据
- 如果我们通过8.3.2绑定了缓冲区索引，接下来就可以将数据绑定到缓冲区了。
#+BEGIN_SRC cpp
void glBufferData (GLenum target, GLsizeiptr size, const void *data, GLenum usage);
#+END_SRC 
- target：缓冲区存放的数据结构。
- size：缓冲区大小，单位：字节。
- data：数据源，其内容将会被复制到缓冲区中。
- usage：缓冲区的使用方法。
- 使用方式主要分为以下三类：
- 只读型(一次修改多次访问)：
  - GL_STATIC_DRAW
  - GL_STATIC_READ
  - GL_STATIC_COPY
- 读写型(多次修改多次访问)：
  - GL_DYNAMIC_DRAW
  - GL_DYNAMIC_READ
  - GL_DYNAMIC_COPY
- 数据流型(一次修改一次访问)：
  - GL_STREAM_DREW
  - GL_STREAM_READ
  - GL_STREAM_COPY
- 这三种类型的后缀DRAW表示数据将被送往GPU渲染、READ表述数据会回传给用户程序、COPY表示其同时具有前两种性质。
- 该方法中的data可以为NULL，如果传入NULL则表示延迟初始化。那么我们应该使用什么方法去再次加载数据呢？接下来介绍一个可以初始化和更新数据的函数：
#+BEGIN_SRC cpp
void glBufferSubData (GLenum target, GLintptr offset, GLsizeiptr size, const void *data);
#+END_SRC 
- offset：缓冲区待修改数据偏移量。
- size：被修改的数据存储字节数。
- data：数据源。
*** 删除缓冲区
- 当我们使用完缓冲区后，应当命令OpenGL销毁这个区域：
#+BEGIN_SRC cpp
void glDeleteBuffers (GLsizei n, const GLuint *buffers);
#+END_SRC 
- n：要删除的缓冲区个数。
- buffers：包含缓冲区索引的数组。
** 映射缓冲区对象
- 当我们需要操作保存在缓冲区的数据时会发现，仅通过8.4节的方式是不能高效的满足需求的。因为我们需要在程序中保存一个引用并不断的调用glBufferSubData方法去更新缓冲区，此时缓冲区的存在完全没有意义。为了你补这个缺陷，OpenGL ES 3.0 中提供了映射缓冲区对象的概念，通过映射缓冲区对象，我们可以直接获得缓冲区中的数据引用——在某些共享内存的架构上，映射缓冲区甚至可以直接得到GPU存储缓冲区地址的直接指针——从而提高性能。
*** 获取缓冲区对象指针
- 我们可以通过以下指令获取指向所有或部分缓存的指针，如果出现错误，该函数将返回NULL。
#+BEGIN_SRC cpp
void* glMapBufferRange (GLenum target, GLintptr offset, GLsizeiptr length, GLbitfield access);
#+END_SRC 
  - target：缓冲区存放的数据结构。
  - offset：缓冲区数据存储的偏移量，单位：字节。
  - length：需要的缓冲区字节数。
  - access：访问方式，是如下方式的一种或组合。
- 缓冲区对象的访问方式：
- 必须包含的选项：
  - GL_MAP_READ_BIT：只读
  - GL_MAP_WRITE_BIT：读写
- 可选的选项：
  - GL_MAP_INVALIDATE_RANGE：告诉OpenGL之前该范围内的数据可能被丢弃。这个标志不得与GL_MAP_READ_BIT结合使用。
  - GL_MAP_INVALIDATE_BUFFER_BIT：告诉OpenGL之前所有的数据都可能被丢弃。这个标志不得与GL_MAP_READ_BIT结合使用。
  - GL_MAP_FLUSH_EXPLICIT_BIT：这个标志只能与GL_MAP_WRITE_BIT一起使用。当使用了此标志之后，需要应用程序明确的使用glFlushMappedBufferRange函数刷新操作。在没有使用此标识位的情况下，glUnmapBuffer函数被调用时会自动刷新。
  - GL_MAP_UNSYNCHRONIZED_BIT：告诉OpenGL不要同步等待glMapBufferRange之前的操作，也就是说指定了该标识之后，会立即从缓冲区中获取引用，无论前面是否还有其他对缓冲区的操作。
*** 取消缓冲区映射
- 当我们不再使用缓冲区映射时，需要使用如下函数解除当前正在使用的映射关系
  - GLboolean glUnmapBuffer (GLenum target);
  - target：缓冲区存放的数据结构。
*** 刷新缓冲区
- 如果我们希望对映射的修改能够立即同步到缓冲区，可以使用glUnmapBuffer解除映射，然后映射中的数据就会被自动刷新到缓冲区中。但是这种方式缺点明显，比如对于连续刷新来说操作繁琐，而且，假如只修改一小部分数据，这种方式却刷新了整个缓冲区。OpenGL ES 3.0提供了部分刷新的方法
#+BEGIN_SRC cpp
void glFlushMappedBufferRange (GLenum target, GLintptr offset, GLsizeiptr length);
#+END_SRC 
- target：缓冲区存放的数据结构。
- offset：缓冲区数据存储的偏移量，单位：字节。
- length：从偏移点开始刷新的长度。
*** 复制缓冲区对象
- OpenGL提供了如下所示的函数来在两个缓冲区之间复制数据
#+BEGIN_SRC cpp
void glCopyBufferSubData (GLenum readTarget, GLenum writeTarget, GLintptr readOffset, GLintptr writeOffset, GLsizeiptr size);
#+END_SRC 
- readTarget：源缓冲区存放的数据结构。
- writeTarget：目标缓冲区存放的数据结构。
- readOffset：源缓冲区偏移量。
- writeOffset：目标缓冲区偏移量。
- size：需要复制的字节长度。


* JSON语法
** （1）JSON简介
-  JSON指的是JavaScript对象表示法（JavaScript Object Notation）
-  JSON是轻量级的文本交换格式，类似于XML，是纯文本
-  JSON独立于语言，具有层级结构（值中存在值）
-  JSON具有自我描述性，更易理解（人类可读）
- JSON使用JavaScript语法来描述数据对象，但是JSON仍然独立于语言和平台。
** （2）相比于XML的不同之处
- JSON比XML更小、更快、更易解析
- 没有结束标签
- 更短，读写速度更快
- 使用数组
- 不使用保留字
** （3）JSON语法规则
- JSON语法是JavaScript对象表示语法的子集
- 数据在 名称/值 对中
- 数据由逗号隔开
- 花括号保存对象
- 方括号保存数组
** （4）JSON 名称/值 对
- JSON 数据的书写格式是：名称/值对。
- 名称/值对包括：字段名称（在双引号中），后面写一个冒号，然后是值：
- “firstName” : “John”
** （5）JSON值
- JSON 值可以是：
  - 数字（整数或浮点数）
  - 字符串（在双引号中）
  - 逻辑值（true 或 false）
  - 数组（在方括号中）
  - 对象（在花括号中）
  - Null
** （6）JSON对象
JSON 对象在花括号中书写：对象可以包含多个名称/值对：
#+BEGIN_SRC json
{ “firstName”:”John” , “lastName”:”Doe” }
#+END_SRC
** （7）JSON 数组
- JSON 数组在方括号中书写：数组可包含多个对象：
#+BEGIN_SRC text
{
“employees”: [
{ “firstName”:”John” , “lastName”:”Doe” },
{ “firstName”:”Anna” , “lastName”:”Smith” },
{ “firstName”:”Peter” , “lastName”:”Jones” }
]
}
#+END_SRC


* GLSL基础语法
** 基本数据类型
- GLSL中的数据类型主要分为标量、向量、矩阵、采样器、结构体、数组、空类型七种类型：
*** 标量
- 标量表示的是只有大小没有方向的量，在GLSL中标量只有bool、int和float三种。对于int，和C一样，可以写为十进制、八进制或者十六进制。对于标量的运算，我们最需要注意的是精度，防止溢出问题。
*** 向量
- 向量我们可以看做是数组，在GLSL通常用于储存颜色、坐标等数据，针对维数，可分为二维、三维和四位向量。针对存储的标量类型，可以分为bool、int和float。共有vec2、vec3、vec4，ivec2、ivec3、ivec4、bvec2、bvec3和bvec4九种类型，数组代表维数、i表示int类型、b表示bool类型。需要注意的是，GLSL中的向量表示竖向量，所以与矩阵相乘进行变换时，矩阵在前，向量在后（与DirectX正好相反）。
- 作为颜色向量时，用rgba表示分量，就如同取数组的中具体数据的索引值。三维颜色向量就用rgb表示分量。比如对于颜色向量vec4 color，color[0]和color.r都表示color向量的第一个值，也就是红色的分量。
- 作为位置向量时，用xyzw表示分量，xyz分别表示xyz坐标，w表示向量的模。三维坐标向量为xyz表示分量，二维向量为xy表示分量。
- 作为纹理向量时，用stpq表示分量，三维用stp表示分量，二维用st表示分量。
*** 矩阵
- 在GLSL中矩阵拥有22、33、4*4三种类型的矩阵，分别用mat2、mat3、mat4表示。我们可以把矩阵看做是一个二维数组，也可以用二维数组下表的方式取里面具体位置的值。
*** 采样器
- 采样器是专门用来对纹理进行采样工作的，在GLSL中一般来说，一个采样器变量表示一副或者一套纹理贴图。所谓的纹理贴图可以理解为我们看到的物体上的皮肤。纹理查找需要指定哪个纹理或者纹理单元将指定查找。
#+BEGIN_SRC cpp
sampler1D // 访问一个一维纹理
sampler2D // 访问一个二维纹理
sampler3D // 访问一个三维纹理
samplerCube // 访问一个立方贴图纹理
sampler1DShadow // 访问一个带对比的一维深度纹理
sampler2DShadow // 访问一个带对比的二维深度纹理
uniform sampler2D grass;
vcc2 coord = vec2(100, 100);
vec4 color = texture2D(grass, coord);
#+END_SRC 
*** 结构体
- 和C语言中的结构体相同，用struct来定义结构体，这是唯一的用户定义类型。
#+BEGIN_SRC cpp
struct light {
    vec3 position;
    vec3 color;
};
light ceiling_light;
#+END_SRC 
*** 数组
- 数组知识也和C中相同，不同的是数组声明时可以不指定大小，但是建议在不必要的情况下，还是指定大小的好。
#+BEGIN_SRC cpp
// 创建一个10个元素的数组
vec4 points[10];
// 创建一个不指定大小的数组
vec4 points[];
points[2] = vec4(1.0); // points现在大小为3
points[7] = vec4(2.0); // points现在大小为8
#+END_SRC 
*** 空类型
- 空类型用void表示，仅用来声明不返回任何值得函数。
- 数据声明示例：
#+BEGIN_SRC cpp
float a = 1.5;
int b = 2;
bool c = true;
vec2 d = vec2(1.1,2.2);
vec3 e = vec3(1.1,2.2,3.3)
vec4 f = vec4(vec3,1.1);
vec4 g = vec4(1.0); //相当于vec(1.0,1.0,1.0,1.0)
vec4 h = vec4(a,a,1.5,a);
mat2 i = mat2(1.1,1.5,2.2,4.4);
mat2 j = mat2(1.8); //相当于mat2(1.8,1.8,1.8,1.8)
mat3 k = mat3(e,e,1.2,1.4,1.6);
#+END_SRC 
** 运算符
- GLSL中的运算符包括（越靠前，运算优先级越高）：
#+BEGIN_SRC text
  - 索引：[ ]
  - 前缀自加和自减：++，–-
  - 一元非和逻辑非：~，！
  - 加法和减法：+，-
  - 等于和不等于：==，！=
  - 逻辑异或：^^
  - 三元运算符号，选择：？ :
  - 成员选择与混合：.
  - 后缀自加和自减：++，–-
  - 乘法和除法：*，/
  - 关系运算符：>，<，=，>=，<=，<>
  - 逻辑与：&&
  - 逻辑或：||
  - 赋值预算：=，+=，-=，*=，/=
#+END_SRC 
*** 类型转换
- GLSL的类型转换与C不同。在GLSL中类型不可以自动提升，比如float a=1;就是一种错误的写法，必须严格的写成float a=1.0，也不可以强制转换，即float a=(float)1; 也是错误的写法，但是可以用内置函数来进行转换，如float a=float(1);还有float a=float(true);（true为1.0，false为0.0）等，值得注意的是，低精度的int不能转换为高精度的float。
*** 限定符
- 观察文首的两段着色器，可以看到对于变量使用了不同的修饰符，以下是GLSL修饰符的主要类型：
#+BEGIN_SRC text
attritude：一般用于各个顶点各不相同的量。如顶点颜色、坐标等。
uniform：一般用于对于3D物体中所有顶点都相同的量。比如光源位置，统一变换矩阵等。
varying：表示易变量，一般用于顶点着色器传递到片元着色器的量。
const：常量。 限定符与java限定符类似，放在变量类型之前，并且只能用于全局变量。在GLSL中，没有默认限定符一说。
#+END_SRC 
*** 流程控制
- GLSL中的流程控制与C中基本相同，主要有条件判断和各种循环：
#+BEGIN_SRC text
if( )、if( )else、if( )else if( )else
while()和dowhile( )
for( ; ; )
break和continue
#+END_SRC 
*** 函数修饰符
- GLSL中也可以定义函数，定义函数的方式也与C语言基本相同。函数的返回值可以是GLSL中的除了采样器的任意类型。对于GLSL中函数的参数，可以用参数用途修饰符来进行修饰，常用函数修饰符如下：
#+BEGIN_SRC text
in：输入参数，无修饰符时默认为此修饰符。
out：输出参数。
inout：既可以作为输入参数，又可以作为输出参数。
#+END_SRC 
*** 浮点精度
- 与顶点着色器不同的是，在片元着色器中使用浮点型时，必须指定浮点类型的精度，否则编译会报错。精度有三种，分别为：
#+BEGIN_SRC text
lowp：低精度。8位。
mediump：中精度。10位。
highp：高精度。16位。
#+END_SRC 
- 当然，也可以在片元着色器中设置默认精度，只需要在片元着色器最上面加上precision <精度> <类型>即可制定某种类型的默认精度。其他情况相同的话，精度越高，画质越好，使用的资源也越多。
*** 着色器代码结构
- GLSL程序的结构和C语言差不多，main()方法表示入口函数，可以在其上定义函数和变量，在main中可以引用这些变量和函数。定义在函数体以外的叫做全局变量，定义在函数体内的叫做局部变量。与高级语言不同的是，变量和函数在使用前必须声明，不能在使用的后面声明变量或者函数。
** GLSL内建变量和内置函数
- 在上面，我们了解了GLSL的基础语法：数据类型、运算符、控制流，接下来，我们要了解另外一些关键的内容：GLSL为方便调用，内置了一些变量和函数，在着色器构造中，少不了这些知识的运用。在上面glsl提供了非常丰富的函数库,供我们使用,这些功能都是非常有用且会经常用到的. 这些函数按功能区分大改可以分成7类:
*** 顶点着色器的内建变量
    #+BEGIN_SRC text
gl_Position：顶点坐标
gl_PointSize：点的大小，没有赋值则为默认值1，通常设置绘图为点绘制才有意义。
    #+END_SRC 
*** 片元着色器的内建变量
#+BEGIN_SRC text
    #+BEGIN_SRC text
gl_FragCoord：当前片元相对窗口位置所处的坐标。
gl_FragFacing：bool型，表示是否为属于光栅化生成此片元的对应图元的正面。 输出变量:
gl_FragColor：当前片元颜色
gl_FragData：vec4类型的数组。向其写入的信息，供渲染管线的后继过程使用。
    #+END_SRC 
- GLSL提供了非常丰富的函数库，供我们使用，这些功能都是非常有用而且经常会使用到的，这些函数按照功能区分大致可以分为7类：
#+BEGIN_SRC text
  - （1）通用函数库
  - （2）三角函数&角度
  - （3）指数函数
  - （4）几何函数
  - （5）矩阵函数
  - （6）向量函数
  - （7）纹理查询函数
#+END_SRC 
  - 纹理采样函数中，3D在OpenGLES2.0并不是绝对支持。我们再次暂时不管3D纹理采样函数。重点只对texture2D函数进行说明。texture2D拥有三个参数，第一个参数表示纹理采样器。第二个参数表示纹理坐标，可以是二维、三维、或者四维。第三个参数加入后只能在片元着色器中调用，且只对采样器为mipmap类型纹理时有效。

* 安卓Camera2 ImageReader
  #+BEGIN_SRC java
 /**
 * 初始化图片读取器
 */
private void initImageReader() {
    // 创建图片读取器,参数为分辨率宽度和高度/图片格式/需要缓存几张图片,我这里写的2意思是获取2张照片
    mImageReader = ImageReader.newInstance(1080, 1920, ImageFormat.YUV_420_888, 2);
    mImageReader.setOnImageAvailableListener(new ImageReader.OnImageAvailableListener() {
        @Override
        public void onImageAvailable(ImageReader reader) {
//        image.acquireLatestImage();//从ImageReader的队列中获取最新的image,删除旧的
//        image.acquireNextImage();//从ImageReader的队列中获取下一个图像,如果返回null没有新图像可用
            Image image = reader.acquireNextImage();
            try {
                File path = new File(Camera2Activity.this.getExternalCacheDir().getPath());
                if (!path.exists()) {
                    Log.e(TAG, "onImageAvailable: 路径不存在");
                    path.mkdirs();
                } else {
                    Log.e(TAG, "onImageAvailable: 路径存在");
                }
                File file = new File(path, "demo.jpg");
                FileOutputStream fileOutputStream = new FileOutputStream(file);
//        这里的image.getPlanes()[0]其实是图层的意思,因为我的图片格式是JPEG只有一层所以是geiPlanes()[0],如果你是其他格式(例如png)的图片会有多个图层,就可以获取指定图层的图像数据　　　　　　　
                ByteBuffer byteBuffer = image.getPlanes()[0].getBuffer();
                byte[] bytes = new byte[byteBuffer.remaining()];
                byteBuffer.get(bytes);
                fileOutputStream.write(bytes);
                fileOutputStream.flush();
                fileOutputStream.close();
                image.close();
            } catch (FileNotFoundException e) {
                e.printStackTrace();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
    }, mChildHandler);
}
  #+END_SRC 
- ImageReader类允许应用程序直接访问呈现表面的图像数据
- 创建ImageReader对像
- ImageReader ir = ImageReader.newInstance(int width, int height, int format, int maxImages);
- 参数：
  - 默认图像的宽度像素
  - 默认图像的高度像素
  - 图像的格式
  - 用户想要读图像的最大数量
- ImageReader类的主要操作:
#+BEGIN_SRC java
getSurface() // 得到一个表面,可用于生产这个ImageReader图像
acquireLatestImage() // 从ImageReader的队列获得最新的图像,放弃旧的图像。
acquireNextImage() // 从ImageReader的队列获取下一个图像
getMaxImages() // 最大数量的图像
getWidth() // 每个图像的宽度,以像素为单位。
getHeight() // 每个图像的高度,以像素为单位。
getImageFormat() // 图像格式。
close() // 释放与此ImageReader相关的所有资源。用完记得关
#+END_SRC 
- ImageReader是一个可以让我们对绘制到surface的图像进行直接操作的类。在这里我们从摄像设备中传入了连续的预览图片，也就是我们在屏幕上看到的画面，它们的格式都是未经压缩的YUV_420_888类型的（同样的如果要操作拍摄后的图片，就要设置成jpeg格式）。我们调用imageReader.acquireLatestImage或者acquireNextImage来获取图像队列中的图片。并进行操作。在这里我利用一个函数将图像压缩后转化成byte[]格式，并调用uploadImg函数上传至服务器。这样，整个摄像头的调用到预览图像的处理也就完成了。想要实现拍照功能也是大同小异，在这里我就不一一贴出了。(这段话来自别处，并不自动对应上面的代码)

** CameraCaptureSession
- CameraCaptureSession 是一个事务，用来向相机设备发送获取图像的请求。
- 主要有 setRepeatingRequest() 和 capture() 方法。setRepeatingRequest() 是重复请求获取图像数据，常用于预览或连拍，capture() 是获取一次，常用于单张拍照。
- CameraCaptureSession 类是一个抽象类，其直接的实现类为 CameraConstrainedHighSpeedCaptureSession。
** 获取 CameraCaptureSession 实例
- 通过 CameraDevice 类的 createCaptureSession() 方法创建，并在回调的 onConfigured(CameraCaptureSession session) 方法中获取实例。
#+BEGIN_SRC java
try {
    mCameraDevice.createCaptureSession(Arrays.asList(mPreviewSurface, mImageReader.getSurface()),
            new CameraCaptureSession.StateCallback() {

        @Override
        public void onConfigured(@NonNull CameraCaptureSession session) {
            mCaptureSession = session;
            // ...（省略）
        }

        @Override
        public void onConfigureFailed(@NonNull CameraCaptureSession session) {
            Log.e(TAG, "ConfigureFailed. session: mCaptureSession");
        }
    }, mBackgroundHandler); // handle 传入 null 表示使用当前线程的 Looper
} catch (CameraAccessException e) {
    e.printStackTrace();
}
#+END_SRC 
- 函数原型：
#+BEGIN_SRC java
createCaptureSession(List outputs, CameraCaptureSession.StateCallback callback, Handler handler)
#+END_SRC 
- 参数说明：
#+BEGIN_SRC text
outputs ： 输出的 Surface 集合，每个 CaptureRequest 的输出 Surface 都应该是 outputs 的一个子元素。例如上例中使用了一个 mPreviewSurface 用于预览的输出，一个 mImageReader.getSurface() 用于拍照的输出。
callback ： 创建会话的回调。成功时将调用 onConfigured(CameraCaptureSession session) 方法。
handler ： 指定回调执行的线程，传 null 时默认使用当前线程的 Looper。
#+END_SRC 
** 内部类
*** CameraCaptureSession.StateCallback 表格，把它们弄好
- 当相机捕捉事务的状态发生变化时，会回调这个类中的相应方法。其中只有 onConfigured 和 onConfigureFailed 两个方法是抽象的（必须重写），其余均有空实现。
|---------------------------------------------------+----------------------------------------------------------|
| 方法名                                            | 描述                                                     |
|---------------------------------------------------+----------------------------------------------------------|
| onConfigureFailed(CameraCaptureSession session)   | 该会话无法按照相应的配置发起请求时回调                   |
| onConfigured(CameraCaptureSession session)        | 相机设备完成配置，并开始处理捕捉请求时回调               |
| onActive(CameraCaptureSession session)            | 在 onConfigured 后执行，即开始真的处理捕捉请求了         |
| onCaptureQueueEmpty(CameraCaptureSession session) | 当相机设备的输入捕捉队列为空时回调                       |
| onClosed(CameraCaptureSession session)            | 当事务关闭时回调                                         |
| onReady(CameraCaptureSession session)             | 每当没有捕捉请求处理时都会回调该方法，即该方法会回调多次 |
|---------------------------------------------------+----------------------------------------------------------|
*** CameraCaptureSession.CaptureCallback
- 当一个捕捉请求发送给相机设备时，可以使用这个类来跟踪各进度。
- 所有方法均有默认的空实现，根据需求重写相应的方法即可。
#+BEGIN_SRC java
// 当相机设备开始为请求捕捉输出图时
onCaptureStarted(CameraCaptureSession session, CaptureRequest request, long timestamp, long frameNumber)	

// 当图像捕获部分进行时就会回调该方法，此时一些(但不是全部)结果是可用的
onCaptureProgressed(CameraCaptureSession session, CaptureRequest request, CaptureResult partialResult)	

// 当图像捕捉完全完成时，并且结果已经可用时回调该方法
onCaptureCompleted(CameraCaptureSession session, CaptureRequest request, TotalCaptureResult result)	

// 对应 onCaptureCompleted 方法，当相机设备产生 TotalCaptureResult 失败时就回调该方法
onCaptureFailed(CameraCaptureSession session, CaptureRequest request, CaptureFailure failure)	

// 当一个捕捉段完成时并且所有 CaptureResult 或者 captureFailure 都通过该监听器返回时被回调，这个方法独立于 CaptureCallback 的其他方法
onCaptureSequenceCompleted(CameraCaptureSession session, int sequenceId, long frameNumber)	

// 当 CaptureResult 或者 captureFailure 没有通过该监听器被返回而被中断时被回调，这个方法同样独立于 CaptureCallback 的其他方法
onCaptureSequenceAborted(CameraCaptureSession session, int sequenceId)	

// 当捕捉的缓冲没有被送到目标 surface 时被回调
onCaptureBufferLost(CameraCaptureSession session, CaptureRequest request, Surface target, long frameNumber)	
#+END_SRC 

* SurfaceView GLSurfaceView TextureView比较与原理：感觉写得很透
- https://www.jb51.net/article/222595.htm 还差几副图，关于案例的部分，改天review再补上
- SurfaceView, GLSurfaceView, SurfaceTexture以及TextureView是Android当中名字比较绕，关系又比较密切的几个类。本文基于Android 5.0(Lollipop)的代码理一下它们的基本原理，联系与区别。
** SurfaceView
- SurfaceView从Android 1.0(API level 1)时就有 。它继承自类View，因此它本质上是一个View。但与普通View不同的是，它有自己的Surface。我们知道，一般的Activity包含的多个View会组成View hierachy的树形结构，只有最顶层的DecorView，也就是根结点视图，才是对WMS可见的。这个DecorView在WMS中有一个对应的WindowState。相应地，在SF中对应的Layer。而SurfaceView自带一个Surface，这个Surface在WMS中有自己对应的WindowState，在SF中也会有自己的Layer。如下图所示：

[[./pic/surfaceview.jpg]]

- 也就是说，虽然在App端它仍在View hierachy中，但在Server端（WMS和SF）中，它与宿主窗口是分离的。这样的好处是对这个Surface的渲染可以放到单独线程去做，渲染时可以有自己的GL context。这对于一些游戏、视频等性能相关的应用非常有益，因为它不会影响主线程对事件的响应。但它也有缺点，因为这个Surface不在View hierachy中，它的显示也不受View的属性控制，所以不能进行平移，缩放等变换，也不能放在其它ViewGroup中，一些View中的特性也无法使用。
** GLSurfaceView
- GLSurfaceView从Android 1.5(API level 3)开始加入，作为SurfaceView的补充。它可以看作是SurfaceView的一种典型使用模式。在SurfaceView的基础上，它加入了EGL的管理，并自带了渲染线程。另外它定义了用户需要实现的Render接口，提供了用Strategy pattern更改具体Render行为的灵活性。作为GLSurfaceView的Client，只需要将实现了渲染函数的Renderer的实现类设置给GLSurfaceView即可。如：
#+BEGIN_SRC java
public class TriangleActivity extends Activity {
    protected void onCreate(Bundle savedInstanceState) {
        mGLView = new GLSurfaceView(this);
        mGLView.setRenderer(new RendererImpl(this));
#+END_SRC 
- 相关类图如下。其中SurfaceView中的SurfaceHolder主要是提供了一坨操作Surface的接口。GLSurfaceView中的EglHelper和GLThread分别实现了上面提到的管理EGL环境和渲染线程的工作。GLSurfaceView的使用者需要实现Renderer接口。

[[./pic/glsurfaceview.jpg]]

** SurfaceTexture
- SurfaceTexture从Android 3.0(API level 11)加入。和SurfaceView不同的是，它对图像流的处理并不直接显示，而是转为GL外部纹理，因此可用于图像流数据的二次处理（如Camera滤镜，桌面特效等）。比如Camera的预览数据，变成纹理后可以交给GLSurfaceView直接显示，也可以通过SurfaceTexture交给TextureView作为View heirachy中的一个硬件加速层来显示。首先，SurfaceTexture从图像流（来自Camera预览，视频解码，GL绘制场景等）中获得帧数据，当调用updateTexImage()时，根据内容流中最近的图像更新SurfaceTexture对应的GL纹理对象，接下来，就可以像操作普通GL纹理一样操作它了。从下面的类图中可以看出，它核心管理着一个BufferQueue的Consumer和Producer两端。Producer端用于内容流的源输出数据，Consumer端用于拿GraphicBuffer并生成纹理。SurfaceTexture.OnFrameAvailableListener用于让SurfaceTexture的使用者知道有新数据到来。JNISurfaceTextureContext是OnFrameAvailableListener从Native到Java的JNI跳板。其中SurfaceTexture中的attachToGLContext()和detachToGLContext()可以让多个GL context共享同一个内容源。

[[./pic/surfacetexture.jpg]]

- Android 5.0中将BufferQueue的核心部分分离出来，放在BufferQueueCore这个类中。BufferQueueProducer和BufferQueueConsumer分别是它的生产者和消费者实现基类（分别实现了IGraphicBufferProducer和IGraphicBufferConsumer接口）。它们都是由BufferQueue的静态函数createBufferQueue()来创建的。Surface是生产者端的实现类，提供dequeueBuffer/queueBuffer等硬件渲染接口，和lockCanvas/unlockCanvasAndPost等软件渲染接口，使内容流的源可以往BufferQueue中填graphic buffer。GLConsumer继承自ConsumerBase，是消费者端的实现类。它在基类的基础上添加了GL相关的操作，如将graphic buffer中的内容转为GL纹理等操作。到此，以SurfaceTexture为中心的一个pipeline大体是这样的：

[[./pic/surfacetexture2.jpg]]

** TextureView
- TextureView在4.0(API level 14)中引入。它可以将内容流直接投影到View中，可以用于实现Live preview等功能。和SurfaceView不同，它不会在WMS中单独创建窗口，而是作为View hierachy中的一个普通View，因此可以和其它普通View一样进行移动，旋转，缩放，动画等变化。值得注意的是TextureView必须在硬件加速的窗口中。它显示的内容流数据可以来自App进程或是远端进程。从类图中可以看到，TextureView继承自View，它与其它的View一样在View hierachy中管理与绘制。TextureView重载了draw()方法，其中主要把SurfaceTexture中收到的图像数据作为纹理更新到对应的HardwareLayer中。SurfaceTexture.OnFrameAvailableListener用于通知TextureView内容流有新图像到来。SurfaceTextureListener接口用于让TextureView的使用者知道SurfaceTexture已准备好，这样就可以把SurfaceTexture交给相应的内容源。Surface为BufferQueue的Producer接口实现类，使生产者可以通过它的软件或硬件渲染接口为SurfaceTexture内部的BufferQueue提供graphic buffer。

[[./pic/textureview.png]]

** 实例解读
- 下面以VideoDumpView.java（位于/frameworks/base/media/tests/MediaDump/src/com/android/mediadump/）为例分析下SurfaceTexture的使用。这个例子的效果是从MediaPlayer中拿到视频帧，然后显示在屏幕上，接着把屏幕上的内容dump到指定文件中。因为SurfaceTexture本身只产生纹理，所以这里还需要GLSurfaceView配合来做最后的渲染输出。
- 首先，VideoDumpView是GLSurfaceView的继承类。在构造函数VideoDumpView()中会创建VideoDumpRenderer，也就是GLSurfaceView.Renderer的实例，然后调setRenderer()将之设成GLSurfaceView的Renderer。
#+BEGIN_SRC java
109    public VideoDumpView(Context context) {
...
116        mRenderer = new VideoDumpRenderer(context);
117        setRenderer(mRenderer);
118    }
#+END_SRC 
- 随后，GLSurfaceView中的GLThread启动，创建EGL环境后回调VideoDumpRenderer中的onSurfaceCreated()。
#+BEGIN_SRC java
519        public void onSurfaceCreated(GL10 glUnused, EGLConfig config) {
...
551            // Create our texture. This has to be done each time the surface is created.
552            int[] textures = new int[1];
553            GLES20.glGenTextures(1, textures, 0);
554
555            mTextureID = textures[0];
556            GLES20.glBindTexture(GL_TEXTURE_EXTERNAL_OES, mTextureID);
...
575            mSurface = new SurfaceTexture(mTextureID);
576            mSurface.setOnFrameAvailableListener(this);
57578            Surface surface = new Surface(mSurface);
579            mMediaPlayer.setSurface(surface);
#+END_SRC 
- 这里，首先通过GLES创建GL的外部纹理。外部纹理说明它的真正内容是放在ion分配出来的系统物理内存中，而不是GPU中，GPU中只是维护了其元数据。接着根据前面创建的GL纹理对象创建SurfaceTexture。流程如下：
- SurfaceTexture的参数为GLES接口函数glGenTexture()得到的纹理对象id。在初始化函数SurfaceTexture_init()中，先创建GLConsumer和相应的BufferQueue，再将它们的指针通过JNI放到SurfaceTexture的Java层对象成员中。
#+BEGIN_SRC java
230static void SurfaceTexture_init(JNIEnv* env, jobject thiz, jboolean isDetached,
231        jint texName, jboolean singleBufferMode, jobject weakThiz)
232{
...
235    BufferQueue::createBufferQueue(&producer, &consumer);
...
242    sp<GLConsumer> surfaceTexture;
243    if (isDetached) {
244        surfaceTexture = new GLConsumer(consumer, GL_TEXTURE_EXTERNAL_OES,
245                true, true);
246    } else {
247        surfaceTexture = new GLConsumer(consumer, texName,
248                GL_TEXTURE_EXTERNAL_OES, true, true);
249    }
...
256    SurfaceTexture_setSurfaceTexture(env, thiz, surfaceTexture);
257    SurfaceTexture_setProducer(env, thiz, producer);
...
266    sp<JNISurfaceTextureContext> ctx(new JNISurfaceTextureContext(env, weakThiz,
267            clazz));
268    surfaceTexture->setFrameAvailableListener(ctx);
269    SurfaceTexture_setFrameAvailableListener(env, thiz, ctx);
#+END_SRC 
- 由于直接的Listener在Java层，而触发者在Native层，因此需要从Native层回调到Java层。这里通过JNISurfaceTextureContext当了跳板。JNISurfaceTextureContext的onFrameAvailable()起到了Native和Java的桥接作用：
#+BEGIN_SRC java
180void JNISurfaceTextureContext::onFrameAvailable()
...
184        env->CallStaticVoidMethod(mClazz, fields.postEvent, mWeakThiz);
#+END_SRC 
- 其中的fields.postEvent早在SurfaceTexture_classInit()中被初始化为SurfaceTexture的postEventFromNative()函数。这个函数往所在线程的消息队列中放入消息，异步调用VideoDumpRenderer的onFrameAvailable()函数，通知VideoDumpRenderer有新的数据到来。 回到onSurfaceCreated()，接下来创建供外部生产者使用的Surface类。Surface的构造函数之一带有参数SurfaceTexture。
#+BEGIN_SRC java
133    public Surface(SurfaceTexture surfaceTexture) {
...
140            setNativeObjectLocked(nativeCreateFromSurfaceTexture(surfaceTexture));
#+END_SRC 
- 它实际上是把SurfaceTexture中创建的BufferQueue的Producer接口实现类拿出来后创建了相应的Surface类。
#+BEGIN_SRC java
135static jlong nativeCreateFromSurfaceTexture(JNIEnv* env, jclass clazz,
136        jobject surfaceTextureObj) {
137    sp<IGraphicBufferProducer> producer(SurfaceTexture_getProducer(env, surfaceTextureObj));
...
144    sp<Surface> surface(new Surface(producer, true));
#+END_SRC 
- 这样，Surface为BufferQueue的Producer端，SurfaceTexture中的GLConsumer为BufferQueue的Consumer端。当通过Surface绘制时，SurfaceTexture可以通过updateTexImage()来将绘制结果绑定到GL的纹理中。 回到onSurfaceCreated()函数，接下来调用setOnFrameAvailableListener()函数将VideoDumpRenderer（实现SurfaceTexture.OnFrameAvailableListener接口）作为SurfaceTexture的Listener，因为它要监听内容流上是否有新数据。接着将SurfaceTexture传给MediaPlayer，因为这里MediaPlayer是生产者，SurfaceTexture是消费者。后者要接收前者输出的Video frame。这样，就通过Observer pattern建立起了一条通知链：MediaPlayer -> SurfaceTexture -> VideDumpRenderer。在onFrameAvailable()回调函数中，将updateSurface标志设为true，表示有新的图像到来，需要更新Surface了。为毛不在这儿马上更新纹理呢，因为当前可能不在渲染线程。SurfaceTexture对象可以在任意线程被创建（回调也会在该线程被调用），但updateTexImage()只能在含有纹理对象的GL context所在线程中被调用。因此一般情况下回调中不能直接调用updateTexImage()。 与此同时，GLSurfaceView中的GLThread也在运行，它会调用到VideoDumpRenderer的绘制函数onDrawFrame()。
#+BEGIN_SRC java
11372        public void onDrawFrame(GL10 glUnused) {
...
377                if (updateSurface) {
...
380                    mSurface.updateTexImage();
381                    mSurface.getTransformMatrix(mSTMatrix);
382                    updateSurface = false;
...
394            // Activate the texture.
395            GLES20.glActiveTexture(GLES20.GL_TEXTURE0);
396            GLES20.glBindTexture(GL_TEXTURE_EXTERNAL_OES, mTextureID);
...
421            // Draw a rectangle and render the video frame as a texture on it.
422            GLES20.glDrawArrays(GLES20.GL_TRIANGLE_STRIP, 0, 4);
...
429                DumpToFile(frameNumber);
#+END_SRC 
- 这里，通过SurfaceTexture的updateTexImage()将内容流中的新图像转成GL中的纹理，再进行坐标转换。绑定刚生成的纹理，画到屏幕上。整个流程如下：
- 最后onDrawFrame()调用DumpToFile()将屏幕上的内容倒到文件中。在DumpToFile()中，先用glReadPixels()从屏幕中把像素数据存到Buffer中，然后用FileOutputStream输出到文件。 上面讲了SurfaceTexture，下面看看TextureView是如何工作的。还是从例子着手，Android的关于TextureView的官方文档(http://developer.android.com/reference/android/view/TextureView.html)给了一个简洁的例子LiveCameraActivity。它它可以将Camera中的内容放在View中进行显示。在onCreate()函数中首先创建TextureView，再将Activity(实现了TextureView.SurfaceTextureListener接口)传给TextureView，用于监听SurfaceTexture准备好的信号。
#+BEGIN_SRC java
protected void onCreate(Bundle savedInstanceState) {
    ...
    mTextureView = new TextureView(this);
    mTextureView.setSurfaceTextureListener(this);
    ...
}
#+END_SRC 
- TextureView的构造函数并不做主要的初始化工作。主要的初始化工作是在getHardwareLayer()中，而这个函数是在其基类View的draw()中调用。TextureView重载了这个函数：
#+BEGIN_SRC java
348    HardwareLayer getHardwareLayer() {
...
358            mLayer = mAttachInfo.mHardwareRenderer.createTextureLayer();
359            if (!mUpdateSurface) {
360                // Create a new SurfaceTexture for the layer.
361                mSurface = new SurfaceTexture(false);
362                mLayer.setSurfaceTexture(mSurface);
363            }
364            mSurface.setDefaultBufferSize(getWidth(), getHeight());
365            nCreateNativeWindow(mSurface);
36367            mSurface.setOnFrameAvailableListener(mUpdateListener, mAttachInfo.mHandler);
368
369            if (mListener != null && !mUpdateSurface) {
370                mListener.onSurfaceTextureAvailable(mSurface, getWidth(), getHeight());
371            }
...
390        applyUpdate();
391        applyTransformMatrix();
392
393        return mLayer;
394    }
#+END_SRC 
- 因为TextureView是硬件加速层（类型为LAYER_TYPE_HARDWARE），它首先通过HardwareRenderer创建相应的HardwareLayer类，放在mLayer成员中。然后创建SurfaceTexture类，具体流程见前文。之后将HardwareLayer与SurfaceTexture做绑定。接着调用Native函数nCreateNativeWindow，它通过SurfaceTexture中的BufferQueueProducer创建Surface类。注意Surface实现了ANativeWindow接口，这意味着它可以作为EGL Surface传给EGL接口从而进行硬件绘制。然后setOnFrameAvailableListener()将监听者mUpdateListener注册到SurfaceTexture。这样，当内容流上有新的图像到来，mUpdateListener的onFrameAvailable()就会被调用。然后需要调用注册在TextureView中的SurfaceTextureListener的onSurfaceTextureAvailable()回调函数，通知TextureView的使用者SurfaceTexture已就绪。整个流程大体如下：
- 注意这里这里为TextureView创建了DeferredLayerUpdater，而不是像Android 4.4(Kitkat)中返回GLES20TextureLayer。因为Android 5.0(Lollipop)中在App端分离出了渲染线程，并将渲染工作放到该线程中。这个线程还能接收VSync信号，因此它还能自己处理动画。事实上，这里DeferredLayerUpdater的创建就是通过同步方式在渲染线程中做的。DeferredLayerUpdater，顾名思义，就是将Layer的更新请求先记录在这，当渲染线程真正要画的时候，再进行真正的操作。其中的setSurfaceTexture()会调用HardwareLayer的Native函数nSetSurfaceTexture()将SurfaceTexture中的surfaceTexture成员（类型为GLConsumer）传给DeferredLayerUpdater，这样之后要更新纹理时DeferredLayerUpdater就知道从哪里更新了。 前面提到初始化中会调用onSurfaceTextureAvailable()这个回调函数。在它的实现中，TextureView的使用者就可以将准备好的SurfaceTexture传给数据源模块，供数据源输出之用。如：
#+BEGIN_SRC java
public void onSurfaceTextureAvailable(SurfaceTexture surface, int width, int height) {
    mCamera = Camera.open();
        ...
        mCamera.setPreviewTexture(surface);
        mCamera.startPreview();
        ...
}
#+END_SRC 
- 看一下setPreviewTexture()的实现，其中把SurfaceTexture中初始化时创建的GraphicBufferProducer拿出来传给Camera模块。
#+BEGIN_SRC java
576static void android_hardware_Camera_setPreviewTexture(JNIEnv *env,
577        jobject thiz, jobject jSurfaceTexture)
...
585        producer = SurfaceTexture_getProducer(env, jSurfaceTexture);
...
594    if (camera->setPreviewTarget(producer) != NO_ERROR) {
#+END_SRC 
- 到这里，一切都初始化地差不多了。接下来当内容流有新图像可用，TextureView会被通知到（通过SurfaceTexture.OnFrameAvailableListener接口）。SurfaceTexture.OnFrameAvailableListener是SurfaceTexture有新内容来时的回调接口。TextureView中的mUpdateListener实现了该接口：
#+BEGIN_SRC java
755        public void onFrameAvailable(SurfaceTexture surfaceTexture) {
756            updateLayer();
757            invalidate();
758        }
#+END_SRC 
- 可以看到其中会调用updateLayer()函数，然后通过invalidate()函数申请更新UI。updateLayer()会设置mUpdateLayer标志位。这样，当下次VSync到来时，Choreographer通知App通过重绘View hierachy。在UI重绘函数performTranversals()中，作为View hierachy的一分子，TextureView的draw()函数被调用，其中便会相继调用applyUpdate()和HardwareLayer的updateSurfaceTexture()函数。
#+BEGIN_SRC java
138    public void updateSurfaceTexture() {
139        nUpdateSurfaceTexture(mFinalizer.get());
140        mRenderer.pushLayerUpdate(this);
141    }
#+END_SRC 
- updateSurfaceTexture()实际通过JNI调用到android_view_HardwareLayer_updateSurfaceTexture()函数。在其中会设置相应DeferredLayerUpdater的标志位mUpdateTexImage，它表示在渲染线程中需要更新该层的纹理。
- 前面提到，Android 5.0引入了渲染线程，它是一个更大的topic，超出本文范围，这里只说相关的部分。作为背景知识，下面只画出了相关的类。可以看到，ThreadedRenderer作为新的HardwareRenderer替代了Android 4.4中的Gl20Renderer。其中比较关键的是RenderProxy类，需要让渲染线程干活时就通过这个类往渲染线程发任务。RenderProxy中指向的RenderThread就是渲染线程的主体了，其中的threadLoop()函数是主循环，大多数时间它会poll在线程的Looper上等待，当有同步请求（或者VSync信号）过来，它会被唤醒，然后处理TaskQueue中的任务。TaskQueue是RenderTask的队列，RenderTask代表一个渲染线程中的任务。如DrawFrameTask就是RenderTask的继承类之一，它主要用于渲染当前帧。而DrawFrameTask中的DeferredLayerUpdater集合就存放着之前对硬件加速层的更新操作申请。
- 当主线程准备好渲染数据后，会以同步方式让渲染线程完成渲染工作。其中会先调用processLayerUpdate()更新所有硬件加速层中的属性，继而调用到DeferredLayerUpdater的apply()函数，其中检测到标志位mUpdateTexImage被置位，于是会调用doUpdateTexImage()真正更新GL纹理和转换坐标。
- 最后，总结下这几者的区别和联系。简单地说，SurfaceView是一个有自己
  Surface的View。它的渲染可以放在单独线程而不是主线程中。其缺点是不能
  做变形和动画。SurfaceTexture可以用作非直接输出的内容流，这样就提供二
  次处理的机会。与SurfaceView直接输出相比，这样会有若干帧的延迟。同时，
  由于它本身管理BufferQueue，因此内存消耗也会稍微大一些。TextureView是
  一个可以把内容流作为外部纹理输出在上面的View。它本身需要是一个硬件加
  速层。事实上TextureView本身也包含了SurfaceTexture。它与
  SurfaceView+SurfaceTexture组合相比可以完成类似的功能（即把内容流上的
  图像转成纹理，然后输出）。区别在于TextureView是在View hierachy中做绘
  制，因此一般它是在主线程上做的（在Android 5.0引入渲染线程后，它是在
  渲染线程中做的）。而SurfaceView+SurfaceTexture在单独的Surface上做绘
  制，可以是用户提供的线程，而不是系统的主线程或是渲染线程。另外，与
  TextureView相比，它还有个好处是可以用Hardware overlay进行显示。
** 再简单地比较一下
*** GlSurfaceView
- GlSurfaceView作为SurfaceView的补充。它可以看作是SurfaceView的一种典型使用模式。在SurfaceView的基础上，它加入了EGL的管理，并自带了渲染线程。另外它定义了用户需要实现的Render接口，可以使用户调用OpenGl自定义渲染的过程。
*** SurfaceTexture
- SurfaceTexture对图像流的处理并不直接显示，而是转为GL外部纹理，因此可用于图像流数据的二次处理（如Camera滤镜，桌面特效等）。
- SurfaceTexture从图像流（来自Camera预览，视频解码，GL绘制场景等）中获得帧数据，当调用updateTexImage()时，根据内容流中最近的图像更新SurfaceTexture对应的GL纹理对象，接下来，就可以像操作普通GL纹理一样操作它了。
- SurfaceTexture.OnFrameAvailableListener用于让SurfaceTexture的使用者知道有新数据到来。
- SurfaceTexture中的attachToGLContext()和detachToGLContext()可以让多个GL context共享同一个内容源。
- SurfaceTexture对象可以在任何线程上创建。 updateTexImage（）只能在包含纹理对象的OpenGL ES上下文的线程上调用。 在任意线程上调用frame-available回调函数，不与updateTexImage（）在同一线程上出现。
*** TextureView
- SurfaceView由于使用的是独立的绘图层，并且使用独立的线程去进行绘制。所以SurfaceView不能进行Transition，Rotation，Scale等变换，这就导致一个问题SurfaceView在滑动的时候，SurfaceView的刷新由于不受主线程控制导致SurfaceView在滑动的时候会出现黑边的情况。
- 为了应对这种情况，所以Android 4.0 google推出了TextureView进行视频播放或者游戏渲染。
- 和SurfaceView不同，它不会在WMS中单独创建窗口，而是作为View hierachy中的一个普通View，因此可以和其它普通View一样进行移动，旋转，缩放，动画等变化。值得注意的是TextureView必须在硬件加速的窗口中。

** 相机滤镜的添加原理
- Camera的setPreview设置Preview输出的时候可以是SurfaceHolder也可以是SurfaceTexture，必须使用SurfaceTexture，只有通过SurfaceTexture获取到Camera输出数据然后一方面通过OpenGl对输出的图像进行加工然后渲染，这一部分的实现可以参考Grafika的MoviePlayer的实现。
  - 当你需要录制添加滤镜的视频的时候，你必须在同一个Opengl环境下也就是EGl对象下面将SurfaceTexture中的texture通过opengl渲染然后送给MediaCodec（Encoder）进行编码然后送入MediaMuxer重新生成MP4视频文件。
- 为什么不用SurfaceHolder作为输出？
  - SurfaceHolder是关联SurfaceView，Camera的数据会直接送给SurfaceView的Surface进行渲染，无法取得图像数据进行再处理。
- 为什么不在Camera的onPreviewFrame(byte[] data, Camera camera)取得图像数据，然后进行处理图像添加滤镜再送去渲染？
  - 原因是效率问题，onPreviewFrame回调中取得的是Yuv格式的图像，而送入渲染的无论是SurfaceView还是Canvas绘制等方法需要的是RGB的颜色格式，这也就意味着要实现这些必须先将Yuv颜色格式转化为RGB，然后将RGB进行添加滤镜处理然后渲染，无论这部分转化处理操作是Java实现还是利用Jni，C来实现（事实上快一点）都会比较慢，同时这些处理是跑在Cpu上面的，内存里面的，也就意味着更大的资源消耗。而利用Opengl对SurfaceTexture中的图像数据进行处理采用GLSL语言，这一切都是在gpu中完成的，所以这无疑是效率最高的方式。
- 为什么使用MediaCodec以及MediaMuxer？
  - 因为MediaCodec是属于硬件编解码，效率最高！但是需要Android 4.3+支持，当然你也可以使用ffmpeg等软编码方案来替代后半部分的录制功能！
- SurfaceView，GlSurfaceView还是TextureView作为Camera Preview的渲染显
  示？
  - 首先由于需要支持Opengl所以GlSurfaceView无疑是最优先的选择，但是TextureView拥有View的平移旋转等特性，更关键的是在ScrollView，ViewPager等控件中只有TextureView才不会出现滑动黑边等问题，所以TextureView才是视频渲染最好的载体。为了使TextureView支持OpenGl，就必须仿照GlSurfaceView自己进行创建EGL，创建GlThread进行渲染，具体的实现可以参考GlTextureViewgles或者开源项目android-openGL-canvas


* Opengl FrameBuffer步骤详解
** 作用
- FrameBuffer Object,也称FBO，离屏渲染，可以摆脱屏幕的束缚，在后台做图像处理。
** 理解
- FrameBuffer和Texture绑定，FrameBuffer犹如画板，而Texture犹如画纸，我们在上面画东西，画完后，我们可以拿Texture去绘制到其他地方上面。
** 代码
- 本章案例效果是在屏幕外绘制一张图片，并保存到本地。
- 由于GL运行需要EGL环境，而GLSurfaceView已经帮我们构建了这样的一个环境，所以我们此次也是在GLSurfaceView上运行，但是不绘制到屏幕上。
- 案例为试验效果，只绘制一帧，所以就放到onDrawFrame上运行，读者之后可以根据自己的需求，处理好相关的生命周期。
#+BEGIN_SRC csharp
public void onDrawFrame(GL10 glUnused) {
    GLES20.glClear(GL10.GL_COLOR_BUFFER_BIT | GL10.GL_DEPTH_BUFFER_BIT);
 
    // 1. 创建FrameBuffer、纹理对象
    createEnv();
    // 2. 配置FrameBuffer相关的绘制存储信息，并且绑定到当前的绘制环境上
    bindFrameBufferInfo();
    // 3. 更新视图区域
    GLES20.glViewport(0, 0, mTextureBean.getWidth(), mTextureBean.getHeight());
    // 4. 绘制图片
    drawTexture();
    // 5. 读取当前画面上的像素信息
    readPixels(0, 0, mTextureBean.getWidth(), mTextureBean.getHeight());
    // 6. 解绑FrameBuffer
    unbindFrameBufferInfo();
    // 7. 删除FrameBuffer、纹理对象
    deleteEnv();
}
#+END_SRC 
- 以上就是关键代码，相比之前其他章节，这里多出了1、2、6、7这几个关键步骤。
** 步骤1. 创建FrameBuffer、纹理对象
   #+BEGIN_SRC csharp
private int[] mFrameBuffer = new int[1];
private int[] mTexture = new int[1];
private void createEnv() {
    // 1. 创建FrameBuffer
    GLES20.glGenFramebuffers(1, mFrameBuffer, 0);
 
    // 2.1 生成纹理对象
    GLES20.glGenTextures(1, mTexture, 0);
    // 2.2 绑定纹理对象
    GLES20.glBindTexture(GLES20.GL_TEXTURE_2D, mTexture[0]);
    // 2.3 设置纹理对象的相关信息：颜色模式、大小
    GLES20.glTexImage2D(GLES20.GL_TEXTURE_2D, 0, GLES20.GL_RGBA,
            mTextureBean.getWidth(), mTextureBean.getHeight(),
            0, GLES20.GL_RGBA, GLES20.GL_UNSIGNED_BYTE, null);
    // 2.4 纹理过滤参数设置
    GLES20.glTexParameterf(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_MIN_FILTER, GLES20.GL_NEAREST);
    GLES20.glTexParameterf(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_MAG_FILTER, GLES20.GL_LINEAR);
    GLES20.glTexParameterf(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_WRAP_S, GLES20.GL_CLAMP_TO_EDGE);
    GLES20.glTexParameterf(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_WRAP_T, GLES20.GL_CLAMP_TO_EDGE);
    // 2.5 解绑当前纹理，避免后续无关的操作影响了纹理内容
    GLES20.glBindTexture(GLES20.GL_TEXTURE_2D, 0);
}
   #+END_SRC 
- 创建纹理和之前的没有差别，而创建Framebuffer也很简单。
** 步骤2. 配置FrameBuffer相关的绘制存储信息，并且绑定到当前的绘制环境上
   #+BEGIN_SRC csharp
private void bindFrameBufferInfo() {
    // 1. 绑定FrameBuffer到当前的绘制环境上
    GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER, mFrameBuffer[0]);
    // 2. 将纹理对象挂载到FrameBuffer上，存储颜色信息
    GLES20.glFramebufferTexture2D(GLES20.GL_FRAMEBUFFER, GLES20.GL_COLOR_ATTACHMENT0,
            GLES20.GL_TEXTURE_2D, mTexture[0], 0);
}
   #+END_SRC 
- *这里先将FrameBuffer绑定到当前的绘制环境上，所以，在没解绑之前，所有的GL图形绘制操作，都不是直接绘制到屏幕上，而是绘制到这个FrameBuffer上！*
- 若想要解绑，想直接绘制到屏幕上，则可以通过
#+BEGIN_SRC csharp
  GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER, 0); // <<<<<===== 0
// 来实现(感觉这里好像写错了吧，可能自己记得不对: 函数仍然是调用GLES20.glBindFramebuffer(..., 0)但参数设置为0
#+END_SRC 
- 第二步是将FrameBuffer和纹理对象相关联，纹理存储绘制到FrameBuffer上的颜色信息，代码也很简单。
** 步骤6. 解绑FrameBuffer
   #+BEGIN_SRC csharp
private void unbindFrameBufferInfo() {
    // 解绑FrameBuffer
    GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER, 0);
}
   #+END_SRC 
- 解绑，之后的绘制操作都是直接绘制到屏幕上。
** 步骤7. 删除FrameBuffer、纹理对象
   #+BEGIN_SRC csharp
private void deleteEnv() {
    GLES20.glDeleteFramebuffers(1, mFrameBuffer, 0);
    GLES20.glDeleteTextures(1, mTexture, 0);
}
   #+END_SRC 
** 注意
- FrameBuffer每次绘制都会得到一个水平镜像翻转的视图，要处理这个问题，可以在绘制的时候添加一个翻转矩阵，或者，用FrameBuffer绘制2次。
** 总结
- 本章使用FrameBuffer实现了离屏渲染，并且将FrameBuffer上的绘制信息保存
  成Bitmap到本地(此处省略，详细可以看GitHub工程)，而FrameBuffer除了这
  个作用外，还可以将离屏渲染好的图片再绘制到屏幕上，而不用绘制到本地，
  毕竟我们绘制后得到一个Texture，那就有发挥的空间。比如我们要做的效果
  是屏幕上画一个背景，背景上有朵花，一共2张图，但背景要做滤镜处理，而
  花不用，那么，我们可以将背景通过FrameBuffer去做滤镜处理，然后得到一
  个纹理，直接绘制到屏幕上，而花直接绘制，那么就得到想要的效果了。具体
  留给读者作为练习题。
** Frame Buffer和Render Buffer
- 关联项目： https://github.com/doggycoder/AndroidOpenGLDemo
- Frame Buffer Object（FBO）即为帧缓冲对象，用于离屏渲染缓冲。相对于其它同类技术，如数据拷贝或交换缓冲区等，使用FBO技术会更高效并且更容易实现。而且FBO不受窗口大小限制。FBO可以包含许多颜色缓冲区，可以同时从一个片元着色器写入。FBO是一个容器，自身不能用于渲染，需要与一些可渲染的缓冲区绑定在一起，像纹理或者渲染缓冲区。
- Render Buffer Object（RBO）即为渲染缓冲对象，分为color buffer(颜色)、depth buffer(深度)、stencil buffer(模板)。
- 在使用FBO做离屏渲染时，可以只绑定纹理，也可以只绑定Render Buffer，也可以都绑定或者绑定多个，视使用场景而定。如只是对一个图像做变色处理等，只绑定纹理即可。如果需要往一个图像上增加3D的模型和贴纸，则一定还要绑定depth Render Buffer。
- 同Texture使用一样，FrameBuffer使用也需要调用GLES20.glGenFrameBuffers生成FrameBuffer，然后在需要使用的时候调用GLES20.glBindFrameBuffer。
*** Frame Buffer只与Texture绑定
- FrameBuffer的创建，Texture的创建及参数设置，代码如下：
#+BEGIN_SRC csharp
GLES20.glGenFramebuffers(1, fFrame, 0);
GLES20.glGenTextures(2, textures, start);
for (int i = 0; i < 2; i++) {
    GLES20.glBindTexture(GLES20.GL_TEXTURE_2D, textures[i]);
    GLES20.glTexImage2D(GLES20.GL_TEXTURE_2D, 0,gl_format, width, height,
        0, gl_format, GLES20.GL_UNSIGNED_BYTE, null);
    GLES20.glTexParameterf(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_MIN_FILTER,GLES20.GL_NEAREST);
    GLES20.glTexParameterf(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_MAG_FILTER,GLES20.GL_LINEAR);
    GLES20.glTexParameterf(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_WRAP_S,GLES20.GL_CLAMP_TO_EDGE);
    GLES20.glTexParameterf(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_WRAP_T,GLES20.GL_CLAMP_TO_EDGE);
}
GLES20.glBindTexture(GLES20.GL_TEXTURE_2D,0);
#+END_SRC 
- 创建完毕后，在渲染时使用：
#+BEGIN_SRC csharp
// 绑定FrameBuffer
GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER, fFrame[0]);
// 为FrameBuffer挂载Texture[1]来存储颜色
GLES20.glFramebufferTexture2D(GLES20.GL_FRAMEBUFFER, GLES20.GL_COLOR_ATTACHMENT0,
    GLES20.GL_TEXTURE_2D, textures[1], 0);
// 绑定FrameBuffer后的绘制会绘制到textures[1]上了
GLES20.glViewport(0,0,w,h);
mFilter.draw();
// 解绑FrameBuffer
GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER,0);
#+END_SRC 
*** Frame Buffer与Texture、Render Buffer绑定
- 有时候只绑定纹理并不能满足我们的要求，当我们需要深度的时候，还需要绑定Render Buffer。在绑定纹理的基础上再绑定RenderBuffer其实也就比较简单了。首先使用前还是先生成RenderBuffer，并为RenderBuffer建立数据存储的格式和渲染对象的尺寸:
#+BEGIN_SRC csharp
// 生成Render Buffer
GLES20.glGenRenderbuffers(1,fRender,0);
// 绑定Render Buffer
GLES20.glBindRenderbuffer(GLES20.GL_RENDERBUFFER,fRender[0]);
// 设置为深度的Render Buffer，并传入大小
GLES20.glRenderbufferStorage(GLES20.GL_RENDERBUFFER,GLES20.GL_DEPTH_COMPONENT16,
    width, height);
// 为FrameBuffer挂载fRender[0]来存储深度
GLES20.glFramebufferRenderbuffer(GLES20.GL_FRAMEBUFFER, GLES20.GL_DEPTH_ATTACHMENT,
    GLES20.GL_RENDERBUFFER, fRender[0]);
// 解绑Render Buffer
GLES20.glBindRenderbuffer(GLES20.GL_RENDERBUFFER,0);
#+END_SRC 
- 然后渲染时，增加深度绑定和解绑：
#+BEGIN_SRC csharp
GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER, frameBufferId);
GLES20.glFramebufferTexture2D(GLES20.GL_FRAMEBUFFER, GLES20.GL_COLOR_ATTACHMENT0,
    GLES20.GL_TEXTURE_2D, textureId, 0);
// 为FrameBuffer挂载fRender[0]来存储深度
GLES20.glFramebufferRenderbuffer(GLES20.GL_FRAMEBUFFER, GLES20.GL_DEPTH_ATTACHMENT,
            GLES20.GL_RENDERBUFFER, fRender[0]);
GLES20.glViewport(0,0,width,height);
mFilter.draw();
GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER,0);
// 解绑Render Buffer
GLES20.glBindRenderbuffer(GLES20.GL_RENDERBUFFER,0);
#+END_SRC 
- 最后的处理
  - 不管是FrameBuffer、RenderBuffer还是Texture不再使用时，都应该删除掉，删除的方法类似：
#+BEGIN_SRC csharp
// 删除Render Buffer
GLES20.glDeleteRenderbuffers(1, fRender, 0);
// 删除Frame Buffer
GLES20.glDeleteFramebuffers(1, fFrame, 0);
// 删除纹理
GLES20.glDeleteTextures(1, fTexture, 0);
#+END_SRC 
* EGL环境
** EGLHelper
- 我们模仿Android中GLSurfaceView里面的EglHelper来做一个自己的MyEglHelper方便未来我们使用GLSurfaceView, OpenGL这一块的开发。
- 大体分为九个步骤
   #+BEGIN_SRC csharp
import android.view.Surface;
import javax.microedition.khronos.egl.EGL;
import javax.microedition.khronos.egl.EGL10;
import javax.microedition.khronos.egl.EGLConfig;
import javax.microedition.khronos.egl.EGLContext;
import javax.microedition.khronos.egl.EGLDisplay;
import javax.microedition.khronos.egl.EGLSurface;

public class MyEglHelper {
    private EGL10 mEgl;
    private EGLDisplay mEglDisplay;
    private EGLContext mEglContext;
    private EGLSurface mEglSurface;

    private void initEgl(Surface surface, EGLContext eglContext) {
        // 1.得到Egl实例
        mEgl = (EGL10) EGLContext.getEGL();

        // 2.得到默认的显示设备(就是窗口)
        mEglDisplay = mEgl.eglGetDisplay(EGL10.EGL_DEFAULT_DISPLAY);
        if (mEglDisplay == EGL10.EGL_NO_DISPLAY) {
            throw new RuntimeException("My  eglGetDisplay failed...");
        }

        // 3.初始化默认显示设备
        int[] version = new int[2]; // 主版本号，次版本号
        if (!mEgl.eglInitialize(mEglDisplay, version)) {
            throw new RuntimeException("My eglInitialize failed...");
        }

        // 4.设置显示设备的属性
        int[] attribes = new int[]{
                EGL10.EGL_RED_SIZE, 8,   // 红
                EGL10.EGL_GREEN_SIZE, 8, // 绿
                EGL10.EGL_BLUE_SIZE, 8,  // 蓝
                EGL10.EGL_ALPHA_SIZE, 8, // 透明度
                EGL10.EGL_DEPTH_SIZE, 8, // 深度 3D相关的
                EGL10.EGL_STENCIL_SIZE, 8,    // 模板
                EGL10.EGL_RENDERABLE_TYPE, 4, // 这个是安卓规定的 用opengl2.0这个版本
                EGL10.EGL_NONE // 到这个NONE 他就知道结尾了
        };
        int[] num_config = new int[1];
        if (!mEgl.eglChooseConfig(mEglDisplay, attribes, null, 1, num_config)) {
            throw new IllegalArgumentException("My eglChooseConfig failed...");
        }
        int numConfigs = num_config[0];
        if (numConfigs <= 0) {
            throw new IllegalArgumentException("My No configs match configSpec...");
        }
        // 5.从系统中获取对应属性的配置
        EGLConfig[] configs = new EGLConfig[numConfigs];
        if (!mEgl.eglChooseConfig(mEglDisplay, attribes, configs, numConfigs, num_config)) {
            throw new IllegalArgumentException("My eglChooseConfig#2 failed...");
        }
        // 6. 创建EglContext
        if (eglContext != null) {
            mEglContext = mEgl.eglCreateContext(mEglDisplay, configs[0], eglContext, null);
        } else {
            mEglContext = mEgl.eglCreateContext(mEglDisplay, configs[0], EGL10.EGL_NO_CONTEXT, null);
        }
        // 7.创建渲染的Surface
        mEglSurface = mEgl.eglCreateWindowSurface(mEglDisplay, configs[0], surface, null);
        // 8.绑定EglContext 和 Surface到显示设备中
        if(!mEgl.eglMakeCurrent(mEglDisplay, mEglSurface, mEglSurface, mEglContext)){
            throw new IllegalArgumentException("My eglMakeCurrent failed...");
        }
        // 9.刷新数据，显示渲染场景
    }
    /**
     * 刷新数据，显示渲染场景
     * @return
     */
    public boolean swapBuffers(){
        if (mEgl != null){
            return mEgl.eglSwapBuffers(mEglDisplay, mEglSurface);
        }else {
            throw new IllegalArgumentException("My eglSwapBuffers failed...");
        }
    }
    /**
     * 释放资源
     */
    public void destroyEgl(){
        if(mEgl != null){
            mEgl.eglMakeCurrent(mEglDisplay, EGL10.EGL_NO_SURFACE, EGL10.EGL_NO_SURFACE, EGL10.EGL_NO_CONTEXT);
            mEgl.eglDestroyContext(mEglDisplay, mEglContext);
            mEglContext = null;
            mEgl.eglDestroySurface(mEglDisplay,mEglSurface);
            mEglSurface = null;
            mEgl.eglTerminate(mEglDisplay);
            mEglDisplay = null;
            mEgl = null;
        }
    }
    public EGLContext getmEglContext() {
        return mEglContext;
    }
}
   #+END_SRC 
** 自定义一个我们自己的GLSurfaceView
#+BEGIN_SRC csharp
public abstract class BaseSurfaceView extends SurfaceView implements SurfaceHolder.Callback {
    
    private Surface surface;
    private EGLContext eglContext;
    private GLThread glThread;
    private GLRender glRender;
    
    public final static int RENDERMODE_WHEN_DIRTY = 0;
    public final static int RENDERMODE_CONTINUOUSLY = 1;
    private int mRenderMode = RENDERMODE_CONTINUOUSLY;

    public BaseSurfaceView(Context context) {
        this(context, null);
    }
    public BaseSurfaceView(Context context, AttributeSet attrs) {
        this(context, attrs, 0);
    }
    public BaseSurfaceView(Context context, AttributeSet attrs, int defStyleAttr) {
        super(context, attrs, defStyleAttr);
        getHolder().addCallback(this);
    }

    public void setRender(GLRender GLRender) {
        this.glRender = GLRender;
    }
    public void setRenderMode(int mRenderMode) {
        if (glRender == null) 
            throw new RuntimeException("must set render before");
        this.mRenderMode = mRenderMode;
    }
    public void setSurfaceAndEglContext(Surface surface, EGLContext eglContext) {
        this.surface = surface;
        this.eglContext = eglContext;
    }

    public EGLContext getEglContext() {
        if (glThread != null)        
            return glThread.getEglContext();
        return null;
    }

    public void requestRender() {
        if (glThread != null) 
            glThread.requestRender();
    }
    @Override public void surfaceCreated(SurfaceHolder holder) {
        if (surface == null) 
            surface = holder.getSurface();
        glThread = new GLThread(new WeakReference<BaseSurfaceView>(this));
        glThread.isCreate = true;
        glThread.start();
    }
    @Override public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {
        glThread.width = width;
        glThread.height = height;
        glThread.isChange = true;
    }
    @Override public void surfaceDestroyed(SurfaceHolder holder) {
        glThread.onDestory();
        glThread = null;
        surface = null;
        eglContext = null;
    }

    public interface GLRender {
        void onSurfaceCreated();
        void onSurfaceChanged(int width, int height);
        void onDrawFrame();
    }

    static class GLThread extends Thread {
        private WeakReference<BaseSurfaceView> wleglSurfaceViewWeakReference;
        private BaseEglHelper baseEglHelper = null;
        private Object object = null;
        private boolean isExit = false;
        private boolean isCreate = false;
        private boolean isChange = false;
        private boolean isStart = false;
        private int width;
        private int height;

        public GLThread(WeakReference<BaseSurfaceView> wleglSurfaceViewWeakReference) {
            this.wleglSurfaceViewWeakReference = wleglSurfaceViewWeakReference;
        }

        @Override public void run() {
            super.run();
            isExit = false;
            isStart = false;
            object = new Object();
            baseEglHelper = new BaseEglHelper();
            baseEglHelper.initEgl(wleglSurfaceViewWeakReference.get().surface, wleglSurfaceViewWeakReference.get().eglContext);
            while (true) {
                if (isExit) {
                    //释放资源
                    release();
                    break;
                }
                if (isStart) {
                    if (wleglSurfaceViewWeakReference.get().mRenderMode == RENDERMODE_WHEN_DIRTY) {
                        synchronized (object) {
                            try {
                                object.wait();
                            } catch (InterruptedException e) {
                                e.printStackTrace();
                            }
                        }
                    } else if (wleglSurfaceViewWeakReference.get().mRenderMode == RENDERMODE_CONTINUOUSLY) {
                        try {
                            Thread.sleep(1000 / 60);
                        } catch (InterruptedException e) {
                            e.printStackTrace();
                        }
                    } else throw new RuntimeException("mRenderMode is wrong value");
                }
                onCreate();
                onChange(width, height);
                onDraw();
                isStart = true;
            }
        }
        private void onCreate() {
            if (isCreate && wleglSurfaceViewWeakReference.get().glRender != null) {
                isCreate = false;
                wleglSurfaceViewWeakReference.get().glRender.onSurfaceCreated();
            }
        }
        private void onChange(int width, int height) {
            if (isChange && wleglSurfaceViewWeakReference.get().glRender != null) {
                isChange = false;
                wleglSurfaceViewWeakReference.get().glRender.onSurfaceChanged(width, height);
            }
        }
        private void onDraw() {
            if (wleglSurfaceViewWeakReference.get().glRender != null && baseEglHelper != null) {
                wleglSurfaceViewWeakReference.get().glRender.onDrawFrame();
                if (!isStart) 
                    wleglSurfaceViewWeakReference.get().glRender.onDrawFrame();
                baseEglHelper.swapBuffers();
            }
        }
        private void requestRender() {
            if (object != null) {
                synchronized (object) {
                    object.notifyAll();
                }
            }
        }
        public void onDestory() {
            isExit = true;
            requestRender();
        }
        public void release() {
            if (baseEglHelper != null) {
                baseEglHelper.destoryEgl();
                baseEglHelper = null;
                object = null;
                wleglSurfaceViewWeakReference = null;
            }
        }
        public EGLContext getEglContext() {
            if (baseEglHelper != null) 
                return baseEglHelper.getmEglContext();
            return null;
        }
    }
}
#+END_SRC 
- 调用EGL
- 在surfaceview Surface创建后，初始化 opengl的shader 创建program 绘制，绘制完毕后，调用eglSwapBuffers(display,surface); 来将opengl绘制的纹理渲染到 窗口。在上面的EGL中已经封装在Draw函数内部了。
- node：渲染的线程 必须 与 EGL的创建，即init必须在同一个线程，否则就会绘制失效。这里主要是因为：eglMakeCurrent(display, surface, surface,context)在里面调用了，当然可以在别的线程先初始化好 context 、display 和context ，然后再渲染线程中 绘制前就调用 eglMakeCurrent。
